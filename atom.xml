<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Eoccc的博客</title>
  
  
  <link href="https://eoccc.gitee.io/atom.xml" rel="self"/>
  
  <link href="https://eoccc.gitee.io/"/>
  <updated>2022-09-09T02:05:51.649Z</updated>
  <id>https://eoccc.gitee.io/</id>
  
  <author>
    <name>Eoccc</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DDD大杂烩</title>
    <link href="https://eoccc.gitee.io/2022/08/21/DDD%E5%A4%A7%E6%9D%82%E7%83%A9/"/>
    <id>https://eoccc.gitee.io/2022/08/21/DDD%E5%A4%A7%E6%9D%82%E7%83%A9/</id>
    <published>2022-08-21T07:25:19.000Z</published>
    <updated>2022-09-09T02:05:51.649Z</updated>
    
    <content type="html"><![CDATA[<p>记录一些DDD的知识，比较碎。</p><span id="more"></span><h1 id="设计的两个阶段"><a class="markdownIt-Anchor" href="#设计的两个阶段"></a> 设计的两个阶段</h1><p>领域驱动设计一般分为两个阶段：</p><blockquote><ol><li>以一种领域专家、设计人员、开发人员都能理解的“通用语言”作为相互交流的工具，在不断交流的过程中发现和挖出一些主要的领域概念，然后将这些概念设计成一个领域模型；</li><li>由领域模型驱动软件设计，用代码来表现该领域模型。领域需求的最初细节，在功能层面通过领域专家的讨论得出。</li></ol></blockquote><h1 id="领域模型"><a class="markdownIt-Anchor" href="#领域模型"></a> 领域模型</h1><p>领域模型具有以下特点：</p><blockquote><ol><li>领域模型是对具有某个边界的领域的一个抽象，反映了领域内用户业务需求的本质；领域模型是有边界的，只反应了我们在领域内所关注的部分；</li><li>领域模型只反映业务，和任何技术实现无关；领域模型不仅能反映领域中的一些实体概念，如货物，书本，应聘记录，地址，等；还能反映领域中的一些过程概念，如资金转账，等；</li><li>领域模型确保了我们的软件的业务逻辑都在一个模型中，都在一个地方；这样对提高软件的可维护性，业务可理解性以及可重用性方面都有很好的帮助；</li><li>领域模型能够帮助开发人员相对平滑地将领域知识转化为软件构造；</li><li>领域模型贯穿软件分析、设计，以及开发的整个过程；领域专家、设计人员、开发人员通过领域模型进行交流，彼此共享知识与信息；因为大家面向的都是同一个模型，所以可以防止需求走样，可以让软件设计开发人员做出来的软件真正满足需求；</li><li>要建立正确的领域模型并不简单，需要领域专家、设计、开发人员积极沟通共同努力，然后才能使大家对领域的认识不断深入，从而不断细化和完善领域模型；</li><li>为了让领域模型看的见，我们需要用一些方法来表示它；图是表达领域模型最常用的方式，但不是唯一的表达方式，代码或文字描述也能表达领域模型；</li><li>领域模型是整个软件的核心，是软件中最有价值和最具竞争力的部分；设计足够精良且符合业务需求的领域模型能够更快速的响应需求变化；</li></ol></blockquote><h1 id="一些基本概念"><a class="markdownIt-Anchor" href="#一些基本概念"></a> 一些基本概念</h1><h2 id="实体"><a class="markdownIt-Anchor" href="#实体"></a> 实体</h2><p>实体（Entity）通常具备唯一id，能够被持久化，具有业务逻辑，对应现实世界业务对象。不要理解成简单的orm的对象。</p><h2 id="值对象"><a class="markdownIt-Anchor" href="#值对象"></a> 值对象</h2><p>值对象（value object）描述事物的对象，可以用来传递参数或对实体进行补充，可以认为是实体的属性，没有唯一id。</p><p>如果值对象是共享的，那么这个值对象的所有属性都应该只读。</p><p>值对象的属性应该尽可能简单，不要引用很多其他对象。</p><h2 id="聚合及聚合根"><a class="markdownIt-Anchor" href="#聚合及聚合根"></a> 聚合及聚合根</h2><p>聚合（aggregation）是用来定义领域边界的领域模式，通过定义清晰的对象所属关系和边界来实现领域的内聚，避免错综复杂的关系网。</p><p>一个聚合是一组对象组成的关系整体，每个聚合都有一个根对象，即聚合根（aggregation root），外部只能通过聚合根访问内部的聚合。</p><p>聚合的划分会直接映射到程序的结构上，DDD推荐按聚合设计子包，每个聚合对应一个子包，内部包括entity、value object、repository、domain等。</p><p>聚合的特点：</p><blockquote><ol><li>每个聚合有一个根和一个边界，边界定义了一个聚合内部有哪些实体或值对象，聚合根是聚合内的某个实体；</li><li>聚合内部的对象之间可以相互引用，但是聚合外部如果要访问聚合内部的对象时，必须通过聚合根访问聚合内的对象，也就是说聚合根是外部可以保持对它的引用的唯一元素；</li><li>聚合内除根以外的其他实体的唯一标识都是本地标识，也就是只要在聚合内部保持唯一即可；</li><li>聚合根负责与外部其他对象打交道并维护自己内部的业务规则；</li><li>聚合内部的对象可以持有其他聚合根的引用；</li><li>删除一个聚合根时必须同时删除该聚合内的所有相关对象。</li></ol></blockquote><h2 id="工厂"><a class="markdownIt-Anchor" href="#工厂"></a> 工厂</h2><p>工厂（Factory）用来封装创建一个复杂对象尤其是聚合时所需的逻辑，作用是将创建对象的细节隐藏起来。客户传递给工厂一些简单的参数，然后工厂可以在内部创建出一个复杂的领域对象然后返回给客户。当创建<strong>复杂的</strong>实体或值对象时建议使用工厂模式。</p><p>一个良好的工厂应该具有以下特点：</p><blockquote><ol><li>每个创建方法都是原子的；</li><li>一个工厂应该只能生产透明状态的对象；</li><li>创建整个实体时满足所有的变量；</li><li>一个工厂创建聚合根，聚合内部的实体通过聚合根调用其他工厂创建。</li></ol></blockquote><h2 id="仓储"><a class="markdownIt-Anchor" href="#仓储"></a> 仓储</h2><p>仓储（Repositories）是用来存储实体的集合。仓储中存储的一定是聚合。</p><p>repository面向的是聚合根，dao面向的是数据访问，dao的方法是细粒度的。客户端应该始终调用领域对象，领域对象再调用dao实现数据库交互。</p><h2 id="服务"><a class="markdownIt-Anchor" href="#服务"></a> 服务</h2><p>服务（Service）只负责协调并委派业务逻辑给领域对象进行处理，其本身并不真正实现业务逻辑，绝大部分的业务逻辑都由领域对象承载和实现。</p><p>Service可与多种组件进行交互，包括：其他的service、领域对象和repository 或 dao。</p><p>服务接口的入参和出参都应该是dto。</p><h2 id="dto"><a class="markdownIt-Anchor" href="#dto"></a> DTO</h2><p>数据传输对象（Data Transfer Object）可以起到隐藏领域细节，帮助实现独立封闭的领域模型的作用。</p><p>DTO与领域对象之间的转换通常由Assembler承担。</p><h2 id="interface"><a class="markdownIt-Anchor" href="#interface"></a> Interface</h2><p>Interface层包含与其他系统/客户进行交互的接口与通信设施，在多数应用里，该层可能提供包括web service、rmi或rest等在内的一种或多种通信接口。该层主要由facade、dto和assembler三类组件构成，三类组件均是典型的j2ee模式。</p><h2 id="application"><a class="markdownIt-Anchor" href="#application"></a> Application</h2><p>Application层中主要组件是service。只负责协调并委派业务逻辑给领域对象进行处理。</p><h2 id="domain"><a class="markdownIt-Anchor" href="#domain"></a> Domain</h2><p>Domain层是整个系统的核心层，该层维护一个使用面向对象技术实现的领域模型，几乎全部的业务逻辑会在该层实现。Domain层包含entity（实体）、value object(值对象)、domain event（领域事件）和repository（仓储）等多种重要的领域组件。</p><h2 id="infrastructure"><a class="markdownIt-Anchor" href="#infrastructure"></a> Infrastructure</h2><p>Infrastructure（基础设施层）为interfaces、application和domain三层提供支撑。所有与具体平台、框架相关的实现会在infrastructure中提供，避免三层特别是domain层掺杂进这些实现，从而“污染”领域模型。Infrastructure中最常见的一类设施是对象持久化的具体实现。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;记录一些DDD的知识，比较碎。&lt;/p&gt;</summary>
    
    
    
    
    <category term="随笔" scheme="https://eoccc.gitee.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="DDD" scheme="https://eoccc.gitee.io/tags/DDD/"/>
    
  </entry>
  
  <entry>
    <title>时间轮</title>
    <link href="https://eoccc.gitee.io/2022/08/10/%E6%97%B6%E9%97%B4%E8%BD%AE/"/>
    <id>https://eoccc.gitee.io/2022/08/10/%E6%97%B6%E9%97%B4%E8%BD%AE/</id>
    <published>2022-08-10T14:52:41.000Z</published>
    <updated>2022-08-21T13:19:47.693Z</updated>
    
    <content type="html"><![CDATA[<p>JDK中Timer和DelayQueue可以实现定时任务和延时操作的功能，但是O(nlogn)的时间复杂度在高并发的时候存在性能瓶颈，因此Kafka、Netty、ZooKeeper、Quartz等组件都使用了时间轮作为定时器。</p><span id="more"></span><h1 id="单层时间轮"><a class="markdownIt-Anchor" href="#单层时间轮"></a> 单层时间轮</h1><h2 id="时间轮的概念"><a class="markdownIt-Anchor" href="#时间轮的概念"></a> 时间轮的概念</h2><p>时间轮( TimingWheel) 是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表( TimerTaskList)。 TimerTaskList 是一个环形的双向链表，链表中的元素即为定时任务( TimerTaskEntry)，其中封装了真正的定时任务 (TimerTask) 。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h5dde2pmc6j20mc0cwq3s.jpg" alt="image-20220820161912938" style="zoom:60%;"><p>时间轮的特征：</p><blockquote><ol><li>时间轮由多个时间格组成，时间格的个数固定为wheelSize</li><li>每个时间格的时间跨度为tickMs</li><li>总的时间跨度interval即为 tickMs * wheelSize</li><li>时间轮当前所处的时间为currentTime，currentTime是tickMs的整数倍</li></ol></blockquote><h2 id="插入任务"><a class="markdownIt-Anchor" href="#插入任务"></a> 插入任务</h2><p>当前时间为currentTime，要插入一个延时时间为delayTime的任务时，会将这个任务插入到当前时间格+(delayTime/tickMs)格中。</p><p>假设时间轮的tickMs为1ms，wheelSize为20。初始时currentTime指向时间格0，此时要插入一个定时为2ms，则会将这个任务插入到时间格2中。</p><p>当时间过了2ms后，currentTime指向了时间格2，需要将时间格2对应的TimTaskList中的任务执行。</p><p>此时又有一个定时为8的任务需要插入，则会插入到时间格10。</p><p>如果此时又要插入一个定时为19ms的任务，则会插入到时间格1。</p><p>但是需要注意，这个<strong>时间轮的interval时间为20ms，即插入的任务的定时不能超过20ms。</strong></p><h2 id="执行任务"><a class="markdownIt-Anchor" href="#执行任务"></a> 执行任务</h2><p>currentTime 可以将整个时间轮划分 为到期部分和未到期部分， currentTime 当前指向的时间格也属于到期部分，表示刚好到期，需 要处理此时间格所对应的 TimerTaskList 中的所有任务。</p><h1 id="多层时间轮"><a class="markdownIt-Anchor" href="#多层时间轮"></a> 多层时间轮</h1><p>当一层时间轮的interval时间不够用时，我们就需要升级到多层时间轮。多层时间轮之间的wheelSize是相同的，下一层时间轮的tickMs是上一层时间轮的interval：</p><blockquote><p>假设第一层的时间轮为：tickMs=1ms、wheelSize=20、inteval=20ms</p><p>则第二层的时间轮为：tickMs=20ms、wheelSize=20、inteval=400ms</p></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h5dddyenmtj20nw0ni760.jpg" alt="image-20220820171558184" style="zoom:50%;"><h2 id="插入任务-2"><a class="markdownIt-Anchor" href="#插入任务-2"></a> 插入任务</h2><p>当一层时间轮的interval不足以容纳一个任务的定时时，就会升级到下一层时间轮。</p><p>在前面的时间轮中，如果我们要插入一个定时为350ms的任务时，第一层时间轮不足以容纳，就会升级到第二层时间轮，最终放到时间格17中（350/40=17）。</p><p>由于最低层的时间轮的tickMs=1，则这个时间轮的精度为1ms。</p><h2 id="执行任务-2"><a class="markdownIt-Anchor" href="#执行任务-2"></a> 执行任务</h2><p>随着时间的流逝，第二层的的currentTiume指向了第17格，即时间走了340ms，对于之前插入的定时为350ms的任务，还剩余10ms，这时还不能执行这个任务。</p><p>此时，会将这个任务从第二层时间轮中取出，然后重新插入一个定时为10ms的任务，最终插入到第一层的时间格10，当currentTime指向第一层的时间格时，才会真正执行这个任务，并将这个任务从时间轮中删除。</p><p>但是如果我们在currentTime=0时，插入了一个定时为200ms的任务，最终插入到了第二层的时间格5，currentTime指向这个时间格的时候，这个任务剩余的时间为0，所以会直接执行，不需要重新插入时间轮。</p><p>另外，在判断一个任务能不能执行的时候，还需要判断时间轮的精度，如果剩余的时间小于时间轮的精度，则会直接执行，否则需要重新插入到时间轮。</p>]]></content>
    
    
    <summary type="html">定时任务、延时操作的定时器实现。</summary>
    
    
    
    
    <category term="技术笔记" scheme="https://eoccc.gitee.io/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>protobuf序列化和反序列化</title>
    <link href="https://eoccc.gitee.io/2022/08/10/protobuf%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <id>https://eoccc.gitee.io/2022/08/10/protobuf%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/</id>
    <published>2022-08-10T10:25:19.000Z</published>
    <updated>2022-08-16T01:55:47.894Z</updated>
    
    <content type="html"><![CDATA[<p>Protobuf是一个很优秀的序列化和反序列化工具，具有较高的效率，并且使用紧凑的二进制码，能够减小内存占用，但是有一些坑，需要我们能够正确的使用。这篇博客就来记录一些Protobuf的使用和一些踩过的坑。</p><span id="more"></span><p>使用protobuf序列化和反序列化时，需要注意：</p><blockquote><ol><li>不支持Map类型，可以包装成其他类型再进行序列化</li><li>添加新字段的时候必须添加到尾部</li><li>不能修改名字</li><li>不能修改类型</li><li>不能删除字段</li></ol></blockquote><p>protobuf使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">ProtoStuffSerializer</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Logger</span> <span class="variable">logger</span> <span class="operator">=</span> LoggerFactory.getLogger(ProtoStuffSerializer.class);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">byte</span>[] EMPTY_BYTES = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> Field sizeField;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">static</span> &#123;</span><br><span class="line">      sizeField = WriteSession.class.getDeclaredField(<span class="string">&quot;size&quot;</span>);</span><br><span class="line">      sizeField.setAccessible(<span class="literal">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 序列化成字节数组</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;all&quot;)</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">byte</span>[] serialize(Object obj) &#123;</span><br><span class="line">    <span class="keyword">if</span> (obj == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> EMPTY_BYTES;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">byte</span>[] bytes;</span><br><span class="line">      Schema&lt;Object&gt; schema = (Schema&lt;Object&gt;) getSchema(obj.getClass());</span><br><span class="line">      <span class="type">LinkedBuffer</span> <span class="variable">buffer</span> <span class="operator">=</span> LinkedBuffer.allocate(<span class="number">4096</span>);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        bytes = ProtobufIOUtil.toByteArray(obj, schema, buffer);</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        buffer.clear();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> bytes;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      logger.error(<span class="string">&quot;protostuff serialize error:&quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> EMPTY_BYTES;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;all&quot;)</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">byte</span>[] serialize(Object obj, <span class="type">byte</span>[] preFix) &#123;</span><br><span class="line">    <span class="keyword">if</span> (obj == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> EMPTY_BYTES;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">Schema</span> <span class="variable">schema</span> <span class="operator">=</span> getSchema(obj.getClass());</span><br><span class="line">      <span class="type">byte</span>[] init = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">8192</span>];</span><br><span class="line">      System.arraycopy(preFix, <span class="number">0</span>, init, <span class="number">0</span>, preFix.length);</span><br><span class="line">      <span class="type">LinkedBuffer</span> <span class="variable">buffer</span> <span class="operator">=</span> LinkedBuffer.wrap(init, <span class="number">0</span>, preFix.length);</span><br><span class="line">      <span class="keyword">final</span> <span class="type">ProtostuffOutput</span> <span class="variable">output</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ProtostuffOutput</span>(buffer);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        schema.writeTo(output, obj);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">NotSupportSerialException</span>(<span class="string">&quot;Serializing to a byte array threw an IOException &quot;</span> +</span><br><span class="line">                                            <span class="string">&quot;(should never happen).&quot;</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">      sizeField.set(output, output.getSize() + <span class="number">4</span>);</span><br><span class="line">      <span class="keyword">return</span> output.toByteArray();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      logger.error(<span class="string">&quot;protostuff serialize error: &quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> EMPTY_BYTES;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 反序列化成对象</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; T <span class="title function_">deserialize</span><span class="params">(<span class="type">byte</span>[] bytes, Class&lt;T&gt; type)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (bytes == <span class="literal">null</span> || bytes.length &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      Schema&lt;T&gt; schema = getSchema(type);</span><br><span class="line">      <span class="type">T</span> <span class="variable">obj</span> <span class="operator">=</span> schema.newMessage();</span><br><span class="line">      ProtobufIOUtil.mergeFrom(bytes, obj, schema);</span><br><span class="line">      <span class="keyword">return</span> obj;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      logger.error(<span class="string">&quot;protostuff deserialize error: &quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> &lt;T&gt; Schema&lt;T&gt; <span class="title function_">getSchema</span><span class="params">(Class&lt;T&gt; aClass)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (aClass == HashMap.class) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">NotSupportSerialException</span>(<span class="string">&quot;not support type:&quot;</span> + aClass);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> RuntimeSchema.getSchema(aClass);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Protobuf是一个很优秀的序列化和反序列化工具，具有较高的效率，并且使用紧凑的二进制码，能够减小内存占用，但是有一些坑，需要我们能够正确的使用。这篇博客就来记录一些Protobuf的使用和一些踩过的坑。&lt;/p&gt;</summary>
    
    
    
    
    <category term="随笔" scheme="https://eoccc.gitee.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="技术笔记" scheme="https://eoccc.gitee.io/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"/>
    
    <category term="踩坑" scheme="https://eoccc.gitee.io/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>开发原则，经验总结</title>
    <link href="https://eoccc.gitee.io/2022/08/07/%E5%BC%80%E5%8F%91%E5%8E%9F%E5%88%99/"/>
    <id>https://eoccc.gitee.io/2022/08/07/%E5%BC%80%E5%8F%91%E5%8E%9F%E5%88%99/</id>
    <published>2022-08-07T14:52:41.000Z</published>
    <updated>2022-08-15T03:27:36.041Z</updated>
    
    <content type="html"><![CDATA[<p>从代码的坏味道中，总结一些开发经验。</p><span id="more"></span><h1 id="字段意义唯一性"><a class="markdownIt-Anchor" href="#字段意义唯一性"></a> 字段意义唯一性</h1><p>设计字段的时候，要尽可能保证字段所代表的逻辑的唯一性，不能一个字段控制了多个功能，否则会导致后期理解代码逻辑的成本急剧增加，并且出问题的概率也会增加，甚至是故障。</p><p>一个bad case：</p><blockquote><p>在我们的代码中存在了这样一个字段：productTypeCode，这个字段最开始是代表了商品类型，后来这个字段在后来的业务中被用作了其他的功能，比如上游系统中对商品进行分类，识别特殊的商品，因此，这个字段在业务代码中不断的被修改，有时候会换成另一个值，有时候加个后缀等等，以至于到了后来，我们都不知道这个字段到底是用来干嘛。</p></blockquote><h1 id="偶发性问题"><a class="markdownIt-Anchor" href="#偶发性问题"></a> 偶发性问题</h1><p>在beta环境偶发的问题，到了线上就一定会爆发，一定要查出根源，不要心存侥幸！不要心存侥幸！不要心存侥幸！</p><h1 id="map使用"><a class="markdownIt-Anchor" href="#map使用"></a> Map使用</h1><p>从map里面取出来的东西一定要进行判空处理，即使当前业务代码里面没有问题，随着代码的迭代，后面还是很有可能会发生NPE。不要成为水管工。</p>]]></content>
    
    
    <summary type="html">从代码的坏味道中，总结一些开发经验。</summary>
    
    
    
    
    <category term="随笔" scheme="https://eoccc.gitee.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="经验总结" scheme="https://eoccc.gitee.io/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>kafka客户端</title>
    <link href="https://eoccc.gitee.io/2022/07/19/kafka%E5%AE%A2%E6%88%B7%E7%AB%AF/"/>
    <id>https://eoccc.gitee.io/2022/07/19/kafka%E5%AE%A2%E6%88%B7%E7%AB%AF/</id>
    <published>2022-07-19T14:52:41.000Z</published>
    <updated>2022-08-23T02:18:29.082Z</updated>
    
    <content type="html"><![CDATA[<p>总结kafka客户端的分区策略、幂等性和事物。</p><span id="more"></span><h1 id="分区分配策略"><a class="markdownIt-Anchor" href="#分区分配策略"></a> 分区分配策略</h1><p>消费者消费消息之前，首先得进行分区分配，kafka提供了三种分区分配策略。</p><ol><li><p>RangeAssignor</p><p>kafka会先按照分区总数和消费者总数进行整除，获得一个跨度，然后按照分区跨度进行平均分配，确保分区尽可能平均的分配给所有的消费者。对于剩余的分区（分区总数 % 消费者总数），则前面的消费者（消费者名称按字典序排序）会多分配一个分区。</p><p>假设 n=分区数/消费者数量， m=分区数%消费者数量，那么前 m 个消费者每个分配 n+1 个 分区，后面的(消费者数量～m)个消费者每个分配 n个分区。</p><p>这种分配方式存在一个问题，如果存在多个topic的分区不是消费者总数的整数倍，那么排在前面的消费者会被多分配多个分区。</p><blockquote><p>假设有两个topic，每个topic有4个分区，分配结果为：</p><p>Consumer0:  t0p0,tp1, t1p0,t1p1</p><p>Consumer1:  t0p2,t0p3, t1p2,t1p3</p><p>但是如果有两个topic，且个topic有3个分区，分配结果为：</p><p>Consumer0:  t0p0,tp1, t1p0,t1p1</p><p>Consumer1:  t0p2,t1p2</p></blockquote></li><li><p>RoundRobinAssignor</p><p>将消费组内所有的消费者及消费者订阅的所有topic的分区按照字典序排序，然后通过轮询的方式将分区依此分配给每个消费者。</p><p>这种分配方式如果消费组内所有的消费者订阅的topic都是相同的，那么分区会被很均匀的分配给每个消费者，但是如果消费者订阅的topic不同，就会导致分配不均匀。</p><blockquote><p>假设消费者C0订阅了主题t0；假设消费者C1订阅了主题t0和t1；假设消费者C2订阅了主题t0，t1和t2。t0、t1、t2的分区数分别为1、2、3。此时分配结果为：</p><p>C0: t0p0</p><p>C1: t1p0</p><p>C2: t1p1, t2p0,t2p1,t2p2</p><p>这种分配方式不完美，因为可以将 t1p1分配给C1。</p></blockquote></li><li><p>StickyAssignor</p><p>这是目前最优秀的分区分配策略。Kafka从0.11x开始引入这种分配策略，尽可能保证：分区分配均匀，分区分配尽可能与上一次分配相同</p><p>再分配的时候，会将需要分配的分区平均的分配给消费者。</p><blockquote><p>假设当前的分区分配为：</p><p>C0: t0p0, t1p1, t3p0</p><p>C1: t0p1, t201, t3p1</p><p>C2: t1p0, t2p1</p><p>消费者C1脱离了消费组，则分配结果为：</p><p>C0: t0p0, t1p1, t3p0, <strong>t201</strong></p><p>C2: t1p0, t2p1, <strong>t0p1, t3p1</strong></p><p>RoundRobinAssignor中提到的例子，使用StickyAssignor的分配结果为：</p><p>C0: t0p0</p><p>C1: t1p0, <strong>t1p1</strong></p><p>C2: t2p0,t2p1,t2p2</p></blockquote></li><li><p>自定义分区分配策略</p><p>实现PartitionAssignor接口。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">PartitionAssignor</span> &#123;</span><br><span class="line">  <span class="comment">//提供订阅的消息</span></span><br><span class="line">  Subscription <span class="title function_">subscription</span><span class="params">(Set&lt;String&gt; topics)</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//实现具体的分配逻辑</span></span><br><span class="line">  Map&lt;String, Assignment&gt; <span class="title function_">assign</span><span class="params">(Cluster metadata, Map&lt;String, Subscription&gt; subscriptions)</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//执行分配的时候会调用这个方法</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">onAssignment</span><span class="params">(Assignment assignment)</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">default</span> <span class="keyword">void</span> <span class="title function_">onAssignment</span><span class="params">(Assignment assignment, <span class="type">int</span> generation)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.onAssignment(assignment);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//这个分配策略的名字</span></span><br><span class="line">  String <span class="title function_">name</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h1 id="幂等"><a class="markdownIt-Anchor" href="#幂等"></a> 幂等</h1><p>kafka开启幂等性功能：</p><blockquote><ol><li>将生产者的<code>enable.idempotence</code>配置为true，默认为false</li><li>生产者客户端的retries必须大于0</li><li><code>max.in.flight.requests.per.connection</code>不能大于5</li><li>acks设置为-1</li></ol></blockquote><p>为了实现幂等性，kafka引入了producer id（PID）和sequence number的概念。kafka会为每个producer分配一个id，每个生产者发送到每个分区的每条消息都有一个序列号，每发送一条消息，&lt;PID，分区&gt;对应的序列号就会加1。</p><p>broker端会为每个&lt;PID，分区&gt;维护一个序列号：</p><blockquote><p>当收到消息时，只有序列号（SN_new）比旧的序列号（SN_old）大1时，即SN_new=SN_old+1，才会接受它</p><p>如果新的序列号小于旧的序列号，则说明这是重复消息，会被丢弃</p><p>如果SN_new&gt;SN_old+1，则说明中间有消息没有写入，出现乱序，即有消息丢失，会抛出OutOfOrderSequenceException异常</p></blockquote><h1 id="事务"><a class="markdownIt-Anchor" href="#事务"></a> 事务</h1><p>要开启事务功能，首先必须开启生产者的幂等性功能。</p><p>通过事务，可以保证跨生产者会话的消息幂等发送和事务恢复。</p><p>需要手动的指定transactionalId，transactionalld与PID一一对应，同时通过一个单调递增的producer epoch保证transactionalld的唯一性。</p><p>Kafka 并不能保证己提交的事务中的所有消息都能够被消费 :</p><blockquote><ol><li>对采用日志压缩策略的主题而言，事务中的某些消息有可能被清理(相同 key 的消息， 后写入的消息会覆盖前面写入的消息)。</li><li>事务中消息可能分布在同一个分区的多个日志分段(LogSegment)中，当老的日志分 段被删除时，对应的消息可能会丢失。</li><li>消费者可以通过 seek()方法访问任意offset的消息，从而可能遗漏事务中的部分消息。</li><li>消费者在消费时可能没有分配到事务内的所有分区，因此它也就不能读取事务中的所有消息。</li></ol></blockquote>]]></content>
    
    
    <summary type="html">kafka客户端的一些细节。</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="kafka" scheme="https://eoccc.gitee.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>软引用和弱的使用场景</title>
    <link href="https://eoccc.gitee.io/2022/07/10/%E8%BD%AF%E5%BC%95%E7%94%A8%E5%92%8C%E5%BC%B1%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/"/>
    <id>https://eoccc.gitee.io/2022/07/10/%E8%BD%AF%E5%BC%95%E7%94%A8%E5%92%8C%E5%BC%B1%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/</id>
    <published>2022-07-09T16:00:00.000Z</published>
    <updated>2022-08-01T04:58:42.278Z</updated>
    
    <content type="html"><![CDATA[<p>我们日常开发中一般不会使用到软引用，因此对这块可能比较陌生，看过一些使用场景，但是过一段时间可能就忘记了，这篇博客就整理一下软引用的使用场景。</p><span id="more"></span><h1 id="软引用"><a class="markdownIt-Anchor" href="#软引用"></a> 软引用</h1><p>软引用的特点：当系统的内存不足时，进行回收，否则不会回收。</p><p>应用场景：</p><blockquote><ol><li>保存网页资源，如图片等</li><li>Spring 缓存配置属性缓存–SoftReferenceConfigurationPropertyCache</li></ol></blockquote><h1 id="弱引用"><a class="markdownIt-Anchor" href="#弱引用"></a> 弱引用</h1><p>软引用的特点：下一次GC的时候会回收</p><p>应用场景：</p><blockquote><ol><li>ThreaLocal中的map实现，此map继承了弱引用WeakReference，防止map中的key引用的对象无法被回收（线程一直处于活跃状态，导致ThreaLocal不会被回收）；</li><li>一些高速缓存场景，缓存仅在GC之间生效。</li></ol></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;我们日常开发中一般不会使用到软引用，因此对这块可能比较陌生，看过一些使用场景，但是过一段时间可能就忘记了，这篇博客就整理一下软引用的使用场景。&lt;/p&gt;</summary>
    
    
    
    
    <category term="随笔" scheme="https://eoccc.gitee.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="技术笔记" scheme="https://eoccc.gitee.io/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>kafka消费者</title>
    <link href="https://eoccc.gitee.io/2022/06/21/kafka%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    <id>https://eoccc.gitee.io/2022/06/21/kafka%E6%B6%88%E8%B4%B9%E8%80%85/</id>
    <published>2022-06-21T14:52:41.000Z</published>
    <updated>2022-08-07T17:14:35.504Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka的消费者负责订阅topic，并从订阅的topic上拉取消息。kafka的消费层还有一个消费组（consumer group），每个consumer都有一个消费组，消息会发给订阅了这个topic的<strong>所有</strong>消费组，并由消费组中的<strong>一个</strong>消费者进行消费。</p><span id="more"></span><h1 id="消费者与消费组"><a class="markdownIt-Anchor" href="#消费者与消费组"></a> 消费者与消费组</h1><p>某个主题中共有4个分区(Partition):P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者(C0、C1、C2和C3)，消费组B中有2个消费者C4和C5)。按照Kafka默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之，每一个分区只能被一个消费组中的一个消费者所消费。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h4ypbl8zq6j20nc0eeab4.jpg" alt="image-20220803203252462" style="zoom:50%;"><p>如果消费者的个数大于分区的个数，则有的消费者会分配不到分区。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h4ypbfi2vtj20my0f2dgw.jpg" alt="image-20220803203324669" style="zoom:40%;"><p>一个消费者只会属于一个消费组，消费模式可以分为点对点模式和发布订阅模式：</p><blockquote><ul><li><p>点对点模式：</p><p>所有消费者都属于同一个消费组，partition会均衡地分配给每一个消费者，从而消息会均衡地发送给消费者，每条消息只会被消费一次</p></li><li><p>发布/订阅模式（广播）：</p><p>每个消费者属于一个单独的消费组，每个消费组都订阅topic，消息会发送给所有的消费组，即一条消息会被每个消费者都消费一遍</p></li></ul></blockquote><h1 id="订阅消息"><a class="markdownIt-Anchor" href="#订阅消息"></a> 订阅消息</h1><p>Kafka一个消费者可以订阅一个或多个消息主题，支持多种订阅消息的方式。</p><ul><li><p>订阅一个或多个topic</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;Str ing&gt; topics)</span></span><br></pre></td></tr></table></figure></li><li><p>根据正则表达式订阅主题</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern)</span></span><br></pre></td></tr></table></figure></li><li><p>订阅指定的分区</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span><br></pre></td></tr></table></figure><p>TopicPartition对象中包含了topic和partation两个参数。</p></li></ul><p>如果我们需要知道某个topic的分区信息，可以通过<code>KafkaConsumer.partitionsFor(String tpoic)</code>进行查询，返回一个<code>List&lt;PartitionInfo&gt;</code>列表，PartitionInfo包含了这个topic的分区信息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Part</span>工tioninfo &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String topic;              <span class="comment">//topic</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> partition;             <span class="comment">//分区</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Node leader;               <span class="comment">//这个分区的leader节点</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span>  Node[] replicas;          <span class="comment">//所有副本 ASR</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span>  Node[] inSyncReplicas;    <span class="comment">//同步副本 ISR</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Node[] offlineReplicas;    <span class="comment">//离线副本 OSR</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="消费消息"><a class="markdownIt-Anchor" href="#消费消息"></a> 消费消息</h1><p>Kafka采用poll的方式从服务端拉取消息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title function_">poll</span><span class="params">(<span class="keyword">final</span> Duration timeout)</span></span><br></pre></td></tr></table></figure><p>ConsumerRecords的内部包括了ConsumerRecord，用来存储一次拉取获得的消息集，提供了一个iterator来遍历消息集内部的消息。</p><p>我们可以通过下面的方法获取一个分区的消息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;ConsumerRecord&lt;K, V&gt; <span class="title function_">records</span><span class="params">(TopicPartition partition)</span></span><br></pre></td></tr></table></figure><p>ConsumerRecord中比较关键的属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerRecord</span>&lt;K, V&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;                 <span class="comment">//主题</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> partition;                <span class="comment">//分区</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> offset;                  <span class="comment">//这个消息在分区中的偏移量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp;               <span class="comment">//时间戳</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType;  <span class="comment">//时间戳的类型，有CreateTime和LogAppendTime两种类型</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedKeySize;        <span class="comment">//key序列化器</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedValueSize;      <span class="comment">//value序列化器</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Headers headers;              <span class="comment">//</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;                        <span class="comment">//消息的key</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;                      <span class="comment">//消息的value</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Optional&lt;Integer&gt; leaderEpoch;<span class="comment">//leader的纪元</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> Long checksum;             <span class="comment">//CRC32校验值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="位移提交"><a class="markdownIt-Anchor" href="#位移提交"></a> 位移提交</h1><p>Kafka中每条消息都有唯一的offset，用来表示消息在分区中的位置。消费者也保存了一个offset，用来记录消费到分区中某个消息所在的位置。</p><p>在旧的消费者客户端中，offset是保存在zookeeper中的，而在新的消费者客户端中，是保存在kafka的内部主题__consumer_offsets中。</p><p>如果消费者当前消费到了x，需要提交的位移为x+1。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h4ypavsxvoj20l80awaaj.jpg" alt="image-20220807151328147" style="zoom:50%;"><p><strong>消费者提交偏移量的时机</strong></p><p>消费者提交偏移量，有可能会造成重复消费和消息丢失现象。</p><blockquote><ul><li>拉取到消息立即提交offset，如果这批消息消费的过程中出现了异常，导致部分消息没有消费，就会导致消息丢失</li><li>消息消费完再提交offset，如果这批消息消费的过程中出现了异常，消费了部分消息，但是由于没有消费完，没有提交offset，就会导致消息重复消费</li></ul></blockquote><p>kafka默认是自动提交的，即定期提交，默认是5m提交一次。<code>enable.auto.commit</code>开启自动提交，<code>auto.commit.interval.ms</code>配置提交的时间。自动提交的操作是在KafkaConsumer#poll()中完成的。消费者每隔5秒就会拉取每个分区中的最小offset进行提交，另外，每次向服务端发起拉取消息的请求的时候，都会检查是否可以提交offset，如果可以，就会提交。</p><p>自动提交存在的问题：</p><blockquote><ul><li>重复消费：消费者拉取了一批消息x+1～x+5，消费到x+3的时候，自动提交了一次offset，这一批消息消费完了，但是拉取消息的时候没有提交offset（条件不满足，还不可以自动提交），然后消费者继续消费，消费到x+7的时候，消费者崩溃了，就需要重新从x+3的offset处开始消费，就会导致重复消费。可以减小自动提交的时间窗口。</li><li>消息丢失：异步拉取消息，并发消费这种情况下会导致消息丢失。比如有一个异步线程一直在拉取消息，然后保存在本地，然后有两个线程并发的消费消息，线程A消费x+1～x+5的batch，线程B消费x+6～x+10的batch，消费者自动提交了x+8的offset，但是线程A才消费到了x+3，这是线程A发生了异常，重新消费的时候，就会从x+6的位置开始消费，x+3～x+5的消息就会丢失，而且x+6～x+10的消息会被重复消费。</li></ul></blockquote><p>kafka可以手动提交偏移量，需要将配置<code>enable.auto.commit</code>关闭，然后使用<code>commitSync()</code>方法提交offset。</p><p>一个示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">ConsumerRecords&lt;String, String&gt; records= consumer.poll(<span class="number">1000</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line"><span class="comment">//do some logical processing .</span></span><br><span class="line">&#125;</span><br><span class="line">  consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="控制消费"><a class="markdownIt-Anchor" href="#控制消费"></a> 控制消费</h1><p>KafkaConsumer可以使用pause方法暂停消费，使用resume方法恢复消费：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">pause</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">resume</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span><br></pre></td></tr></table></figure><p>关闭客户端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(Duration timeout)</span></span><br></pre></td></tr></table></figure><h1 id="指定消费位移"><a class="markdownIt-Anchor" href="#指定消费位移"></a> 指定消费位移</h1><p>当一个新的消费组建立的时候，或订阅一个新的tpoic的时候，或当__consumer_offsets主题中关于这个消费组的偏移量消息过期后，没有可以查找的offset，这时会根据消费者的<code>auto.offset.reset</code>配置来决定从什么地方开始消费：</p><blockquote><ul><li>latest：默认值，从下一条写入的消息开始消费</li><li>earliest：从起始处开始消费</li><li>none：抛出NoOffsetForPartitionException异常</li></ul></blockquote><p>kafka还可以通过seek()方法，更细粒度的从指定的位置开始消费。seek()只能重置分配到分区的消费者的位置，所以在重置之前，还得先poll()一次。</p><p>kafka还提供了两个快速seek的方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">seekToBeginning</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">seekToEnd</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span><br></pre></td></tr></table></figure><p>另外可以通过offsetsForTimes方法获取指定时间的offset：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Map&lt;TopicPartition, OffsetAndTimestamp&gt; <span class="title function_">offsetsForTimes</span><span class="params">(Map&lt;TopicPartition, Long&gt; timestampsToSearch)</span></span><br></pre></td></tr></table></figure><p>kafka有再均衡监听器ConsumerRebalanceListener，可以在再均衡之前和重新分配分区之后做一些操作，如在再均衡之前提交当前的offset</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ConsumerRebalanceListener</span> &#123;</span><br><span class="line">  <span class="comment">//再均衡之前调用</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; var1)</span>;</span><br><span class="line"><span class="comment">//重新分配分区之后调用</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; var1)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="消费者拦截器"><a class="markdownIt-Anchor" href="#消费者拦截器"></a> 消费者拦截器</h1><p>消费者拦截器可以在消费消息或者提交偏移量的时候做一些操作，实现ConsumerInterceptor接口即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ConsumerInterceptor</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Configurable</span>, AutoCloseable &#123;</span><br><span class="line">  <span class="comment">//消费消息之前会调用这个方法</span></span><br><span class="line">  <span class="comment">//我们可以通过这个方法修改消息，或者做一些过滤等</span></span><br><span class="line">  ConsumerRecords&lt;K, V&gt; <span class="title function_">onConsume</span><span class="params">(ConsumerRecords&lt;K, V&gt; var1)</span>;</span><br><span class="line">  <span class="comment">//提交偏移量之前会调用</span></span><br><span class="line">  <span class="comment">//我们可以通过这个方法获取一些偏移量提交的细节</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">onCommit</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; var1)</span>;</span><br><span class="line">  <span class="comment">//关闭的时候会调用</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Kafka的消费者负责订阅topic，并从订阅的topic上拉取消息。kafka的消费层还有一个消费组（consumer group），每个consumer都有一个消费组，消息会发给订阅了这个topic的&lt;strong&gt;所有&lt;/strong&gt;消费组，并由消费组中的&lt;strong&gt;一个&lt;/strong&gt;消费者进行消费。&lt;/p&gt;</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="kafka" scheme="https://eoccc.gitee.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>缓存淘汰算法总结</title>
    <link href="https://eoccc.gitee.io/2022/06/20/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://eoccc.gitee.io/2022/06/20/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</id>
    <published>2022-06-19T16:00:00.000Z</published>
    <updated>2022-08-12T08:26:23.201Z</updated>
    
    <content type="html"><![CDATA[<p>以前看到过一些缓存淘汰的算法，觉得很是精妙，比如大名鼎鼎的LRU、LFU、FIFO，以及mysql版的LRU，redis的缓存淘汰策略，GuavaCache的缓存淘汰算法，等等。写篇博客总结一下～～</p><span id="more"></span><h1 id="lru"><a class="markdownIt-Anchor" href="#lru"></a> LRU</h1><p>LRU(Least Recently Used)：最近最少使用算法。</p><p>核心思想：这种算法认为最近使用的数据是热点数据，很有可能被再次使用，而最近很少使用的数据是冷门数据，很有可能不再使用。当缓存容量满了的时候，优先淘汰最近最少使用的数据。</p><p>缓存满的状态下：</p><blockquote><ol><li>读取一条缓存，将更新这条缓存的使用时间</li><li>插入一条缓存，将移除时间最远的一条缓存</li></ol></blockquote><h2 id="lru实现"><a class="markdownIt-Anchor" href="#lru实现"></a> LRU实现</h2><p>LRU可以使用LinkedHashMap实现，从而达到O(1)的查询复杂度和更新复杂度。Hash实现O(1)的查询复杂度，链表维护缓存的新鲜度，靠近链表头部的数据是冷门数据，靠近链表尾部的数据是热点数据。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h4a9md6paaj20su0ay0tk.jpg" alt="image-20220713094354095" style="zoom:50%;"><p><em>LinkedHashMap提供了accessOrder选项，设置为true的时，访问node会将node移动到链表尾部，设置为false时会将插入的node放在链表尾部。</em></p><p><em>LinkedHashMap重写了HashMap的newNode方法，put的时候会将新创建的node放到node的尾部。</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Node&lt;K,V&gt; <span class="title function_">newNode</span><span class="params">(<span class="type">int</span> hash, K key, V value, Node&lt;K,V&gt; e)</span> &#123;</span><br><span class="line">    LinkedHashMap.Entry&lt;K,V&gt; p =</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">LinkedHashMap</span>.Entry&lt;&gt;(hash, key, value, e);</span><br><span class="line">    linkNodeLast(p);</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="lru查询"><a class="markdownIt-Anchor" href="#lru查询"></a> LRU查询</h3><p>当缓存中不包含查询的key时，返回空。</p><p>当查询到数据时，需要将缓存移动到链表的尾部。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h4a9m9z2lqj20tg0bewfg.jpg" alt="image-20220713094854303" style="zoom:50%;"><h3 id="lru更新"><a class="markdownIt-Anchor" href="#lru更新"></a> LRU更新</h3><p>缓存容量未满时：</p><ul><li>缓存中不包含key，直接在链表的尾部插入新的缓存</li><li>缓存中包含key，更新缓存的value，并移动到链表的尾部</li></ul><p>缓存容量满时：</p><ul><li>缓存中不包含key，移除链表头部的数据，然后在链表的尾部插入新的数据</li><li>缓存中包含key，更新缓存的value，并移动到链表的尾部</li></ul><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h4a9m9moz2j20tg0bewfg.jpg" alt="image-20220713095155195" style="zoom:50%;"><h1 id="lfu"><a class="markdownIt-Anchor" href="#lfu"></a> LFU</h1><p>LFU(Least Frequently Used)：最近最不常用算法，根据数据的历史访问频率来淘汰数据。</p><p>核心思想：最近使用频率高的数据很大概率将会再次被使用，而最近使用频率低的数据很大概率不会再使用。淘汰使用频率最小的数据。</p><h2 id="lfu实现"><a class="markdownIt-Anchor" href="#lfu实现"></a> LFU实现</h2><p>LFU也使用hash+链表的实现，同样可以基于LinkedHashMap实现，但是要重写get方法。</p><h3 id="lfu查找"><a class="markdownIt-Anchor" href="#lfu查找"></a> LFU查找</h3><p>在LFU中查找并访问一条数据后，会：</p><blockquote><ol><li>将这个node的访问频率加1</li><li>将这个node移动到相同频率的node的前面</li></ol></blockquote><p>如：A(10)-&gt;B(9)-&gt;C(9)-&gt;D(8)</p><p>当D被访问后，会变成：A(10)-&gt;D(9)-&gt;B(9)-&gt;C(9)</p><h3 id="lfu插入"><a class="markdownIt-Anchor" href="#lfu插入"></a> LFU插入</h3><p>当插入一条缓存时：</p><blockquote><ol><li>如果缓存容量未满，将node放到链表尾部</li><li>如果缓存容量满了，移除链表尾部的一个node，然后将新的node放到链表尾部</li></ol></blockquote><h1 id="arc"><a class="markdownIt-Anchor" href="#arc"></a> ARC</h1><p>ARC(Adaptive Replacement Cache): 自适应缓存替换算法，它结合了LRU与LFU，来获得可用缓存的最佳使用。</p><p>ARC将整个Cache分成四部分：</p><blockquote><ul><li>最近最多使用的缓存链表，LRU链表</li><li>最近最频繁使用的缓存链表，LFU链表</li><li>最近从LRU表中淘汰的缓存链表，ghost LRU链表，只存储key，不存储真正的数据</li><li>最近从LFU表中淘汰的缓存链表，ghost LFU链表，只存储key，不存储真正的数据</li></ul></blockquote><p>初始时，LRU和LFU的空间各占一半，后续会动态适应调整partion的位置。</p><h3 id="arc查找"><a class="markdownIt-Anchor" href="#arc查找"></a> ARC查找</h3><p>先从LRU缓存中查找，如果命中LRU中的缓存，说明这个缓存时访问频率高的缓存，将node移动到LFU缓存中：</p><blockquote><ol><li>如果在LRU链表中查找到缓存，将这个node移动到LFU链表中。<ul><li>如果LFU链表没满，直接追加到尾部</li><li>如果LFU链表满了，将LFU尾部的node的key移动到ghost LFU链表中，再将新的node追加进去</li><li>ghost LFU链表按照LFU算法进行淘汰</li></ul></li><li>如果在LRU链表中查找到缓存，从LFU中查找</li><li>如果LRU缓存和LFU缓存都没有查找到，从磁盘加载数据<ul><li>如果缓存的key存在与ghost LRU中，则将LRU链表的长度加1，LFU链表的长度减1，将缓存插入到LRU链表中</li><li>如果缓存的key存在与ghost LFU中，则将LFU链表的长度加1，LRU链表的长度减1，将缓存插入到LFU链表中</li></ul></li></ol></blockquote><h3 id="arc插入"><a class="markdownIt-Anchor" href="#arc插入"></a> ARC插入</h3><p>任何缓存的插入都要先插入到LRU缓存中：</p><blockquote><ol><li><p>如果LRU缓存没有满，直接将缓存追加到LRU缓存的尾部</p></li><li><p>如果LRU缓存满了：</p><ul><li><p>将LRU缓存的头部节点移动到ghost LRU中</p></li><li><p>将新的缓存插入到LRU的尾部</p></li><li><p>ghost LRU按照LRU算法进行淘汰</p></li></ul></li></ol></blockquote><h1 id="w-tinylfu"><a class="markdownIt-Anchor" href="#w-tinylfu"></a> W-TinyLFU</h1><p>W-TinyLFU（Window Tiny Least Frequently Used）是对LFU的的优化和加强。</p><p>W-TinyLFU算法分为三个区域，WindowLRU区域、TinyLFU区域、SLRU-主Cache区域：</p><blockquote><ol><li><p>缓存插入时，先进入WindowLRU区</p></li><li><p>WindowLRU区淘汰的数据进入TinyLFU区</p><p>被WindowLRU区淘汰，说明这个数据不是最近需要访问的数据，但是有可能其访问频率很高，进入TinyLFU区</p></li><li><p>TinyLFU区满了以后，将频率最高的数据移动到SLRU区</p></li><li><p>SLRU区满了以后，将淘汰的数据重新放到TinyLFU区</p></li></ol></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h4bii54r6ij21ia0nywgr.jpg" alt="img" style="zoom:35%;"><p>另外，LFU通常使用一个map来保存key的访问频率，由于使用map，就会产生hash冲突。caffeine使用了Count-Min Sketch算法存储访问频率。</p><p>Count-Min Sketch算法类似于布隆过滤器，采用4个hash函数进行hash计算，取得四个位置的频率后，取最小值。另外，Count-Min Sketch算法使用四个数组来保存频率。</p><p><img src="/.io//34d432e4f2cd21c4ef18c76074940f81-20220718234745930.png" alt="img"></p><p>但是存在一个问题，如果一个访问频率超级高的数据，突然没有用了，会一直在SLRU区和TinyLFU区中倒腾，就是不能被淘汰，因此caffeine还提供了三种驱逐策略：</p><blockquote><ol><li><p>基于大小：</p><p>可以使用Caffeine.maximumSize(long)方法来指定缓存的最大容量。当缓存超出这个容量的时候，会使用<a href="https://github.com/ben-manes/caffeine/wiki/Efficiency">Window TinyLfu策略</a>来删除缓存。</p></li><li><p>基于时间：</p><ul><li>expireAfterAccess(long, TimeUnit)：在最后一次访问或者写入后开始计时，在指定的时间后过期。</li><li>expireAfterWrite(long, TimeUnit)： 在最后一次写入缓存后开始计时，在指定的时间后过期。</li><li>expireAfter(Expiry)： 自定义策略，过期时间由Expiry实现独自计算。</li></ul></li><li><p>基于引用：没有引用指向数据时，才会驱逐缓存。</p></li></ol></blockquote><p>**一点想法：**对于某些缓存在一定时间内访问的频率很高，积攒了很高的访问频率，导致这些缓存长期存在而不能被淘汰，可以定时对缓存的使用频率进行平滑处理，如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newFrequency = h(oldFrequency) </span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">缓存淘汰算法LRU、LFU、W-TinyLFU等</summary>
    
    
    
    
    <category term="随笔" scheme="https://eoccc.gitee.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="技术笔记" scheme="https://eoccc.gitee.io/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>kafka生产者</title>
    <link href="https://eoccc.gitee.io/2022/06/19/kafka%E7%94%9F%E4%BA%A7%E8%80%85/"/>
    <id>https://eoccc.gitee.io/2022/06/19/kafka%E7%94%9F%E4%BA%A7%E8%80%85/</id>
    <published>2022-06-19T14:52:41.000Z</published>
    <updated>2022-08-01T12:22:44.144Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka的生产者发送数据时，先将数据缓存到记录收集器RecordAccumulator中，再由发送线程Sender将消息批量地发送给服务端。</p><span id="more"></span><p>Kafka生产者的客户端是KafkaProducer。发送消息的入口是KafkaProducer.send，提供了同步和异步的方式，异步的方式支持回调功能：</p><blockquote><p>同步：send(ProducerRecord&lt;K, V&gt; record)</p><p>异步：send(ProducerRecord&lt;K, V&gt; record, Callback callback)</p></blockquote><img src="/.io//image-20220731211734632.png" alt="image-20220731211734632" style="zoom:45%;"><h1 id="序列化"><a class="markdownIt-Anchor" href="#序列化"></a> 序列化</h1><p>Kafka在发送消息之前，会先进行序列化，key和value可以使用不同的序列化器：</p><blockquote><ul><li>key的序列化器通过<code>key.serializer</code>配置，默认使用JsonSerializer</li><li>key的序列化器通过<code>value.serializer</code>配置，默认使用JsonSerializer</li></ul></blockquote><p>JsonSerializer底层使用的是<code>com.fasterxml.jackson.databind.ObjectMapper</code>进行序列化。</p><h1 id="选择分区"><a class="markdownIt-Anchor" href="#选择分区"></a> 选择分区</h1><p>Kafka的消息分为有key和没有key两种，通常情况下是没有key的。针对两种情况，有不同的分区逻辑。</p><p><strong>有key的消息</strong></p><p>对于有key的消息，kafka会根据key进行散列，key相同的消息会发送到相同的分区中。</p><p><strong>没有key的消息</strong></p><p>当消息没有指定key时，如果是第一次向服务端发送消息（这个topic还没有分配过分区），则随机分配一个分区，否则采用轮询的方式分配分区。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String topic, Object key, <span class="type">byte</span>[] keyBytes, Object value, <span class="type">byte</span>[] valueBytes, Cluster cluster)</span> &#123;</span><br><span class="line">  List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">  <span class="type">int</span> <span class="variable">numPartitions</span> <span class="operator">=</span> partitions.size();</span><br><span class="line">  <span class="comment">// 没有key的时候，随机分配分区</span></span><br><span class="line">  <span class="keyword">if</span> (keyBytes == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="comment">// 选择分区</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">nextValue</span> <span class="operator">=</span> nextValue(topic);</span><br><span class="line">    List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">    <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">part</span> <span class="operator">=</span> Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">      <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">      <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 有key的时候直接根据key的hash值进行散列</span></span><br><span class="line">    <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">    <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="title function_">nextValue</span><span class="params">(String topic)</span> &#123;</span><br><span class="line">  <span class="type">AtomicInteger</span> <span class="variable">counter</span> <span class="operator">=</span> topicCounterMap.get(topic);</span><br><span class="line">  <span class="keyword">if</span> (<span class="literal">null</span> == counter) &#123;</span><br><span class="line">    <span class="comment">// 如果这个topi是第一次推送消息，随机分配一个分区</span></span><br><span class="line">    counter = <span class="keyword">new</span> <span class="title class_">AtomicInteger</span>(ThreadLocalRandom.current().nextInt());</span><br><span class="line">    <span class="type">AtomicInteger</span> <span class="variable">currentCounter</span> <span class="operator">=</span> topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">    <span class="keyword">if</span> (currentCounter != <span class="literal">null</span>) &#123;</span><br><span class="line">      counter = currentCounter;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 轮询分配分区</span></span><br><span class="line">  <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>直接指定分区</strong></p><p>Kafka支持直接指定分区，可以在创建ProducerRecord的时候，直接指定分区。ProducerRecord提供了多个指定分区的构造方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="type">byte</span>[] serializedKey, <span class="type">byte</span>[] serializedValue, Cluster cluster)</span> &#123;</span><br><span class="line">  <span class="type">Integer</span> <span class="variable">partition</span> <span class="operator">=</span> record.partition();</span><br><span class="line">  <span class="comment">// 优先使用指定的分区</span></span><br><span class="line">  <span class="keyword">return</span> partition != <span class="literal">null</span> ?</span><br><span class="line">    partition :</span><br><span class="line">  partitioner.partition(</span><br><span class="line">    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>**注意：**分配分区的时候，会过滤掉不健康的分区。</p><p>kafka节点健康的标准：</p><ol><li>存在于ISR集合中（保持正常的同步）</li><li>于zookeeper保持心跳（健康检查）</li></ol><h1 id="客户端消息收集器"><a class="markdownIt-Anchor" href="#客户端消息收集器"></a> 客户端消息收集器</h1><p>Kafka的生产者发送数据时，先将数据缓存到记录收集器RecordAccumulator中，再由发送线程Sender将消息批量地发送给服务端。</p><p>每个分区都有一个双端队列来缓存客户端的消息，队列中的每个元素是一个批记录（ProducerBatch），如果一个批记录满了，就会创建一个新的批记录，并将已经满的批记录交给sender线程发送到服务端。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RecordAccumulator.<span class="type">RecordAppendResult</span> <span class="variable">result</span> <span class="operator">=</span> accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                    serializedValue, headers, interceptCallback, remainingWaitMs);</span><br><span class="line"><span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">  <span class="comment">// 如果批记录满了，唤醒sender线程</span></span><br><span class="line">  <span class="built_in">this</span>.sender.wakeup();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>批记录的大小通过<code>batch.size</code>来配置，默认是16kb。如果一条消息的大小超过了16kb，会创建一个能够容纳这条消息的批记录。</p><p>另外，如果一个批记录很长时间没有满，sender线程会定时的将批记录发送给服务端，避免过长的延时。延时通过<code>linger.ms</code>来配置，默认是0ms，即有消息就会马上发送到服务端。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h4qf68kkg6j20zu0bi0uy.jpg" alt="image-20220727101124764" style="zoom:50%;"><h1 id="客户端消息发送线程"><a class="markdownIt-Anchor" href="#客户端消息发送线程"></a> 客户端消息发送线程</h1><p>Kafka发送消息时，为了减少网络的开销，会将属于一个节点的所有partation的消息放在一个批次，同时进行发送。如果我们有两台服务器，topic有6个partation，每台服务器有3个partation，如果迭代每个partation的批记录，直接发送到主副本节点，则会有6次请求；如果把属于同一个节点的所有partation放在一起发送，就只会有2次请求。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h4qf7366v4j20jg0ciabe.jpg" alt="image-20220727203032416" style="zoom:50%;"><p>消息发送线程发送消息的步骤：</p><blockquote><ol><li>获取可以发送的批记录（每个批记录属于一个partation）</li><li>遍历每个批记录，获取每个批记录对应的主副本节点：nodeId</li><li>将所有的批记录以nodeId为key，group成一个map：modeId -&gt; List<ProducerBatch></ProducerBatch></li><li>发送消息到每个主副本节点</li></ol></blockquote><p>客户端发送完消息后，会执行<code>KafkaClient#poll</code>方法，执行回调方法以及一些后续的处理。回调方法时保存在ClientRequest中的，为了在收到服务端返回后能够执行回调方法，发送线程会保存目标节点到客户端请求的映射关系。</p><ul><li><p>**不需要响应的流程 ：**开始发送请求→添加客户端请求到队列→发送请求→请求发送成功→从 队列中删除发送请求→构造客户端响应。</p></li><li><p>**需要晌应的流程：**开始发送请求→添加客户端请求到队列→发送请求→请求发送成功→等待 接收响应→接收响应→接收到完整的响应→从队列中删除客户端请求→构造客户端响应。</p></li></ul><p>整体流程：</p><blockquote><ol><li>KafkaProducer将消息存到消息收集器RecordAccumulator中</li><li>Sender从RecordAccumulator获取消息</li><li>Sender将需要发送的批记录根据目标节点进行分类</li><li>Sender创建ClientRequest</li><li>Sender调用KafkaClient.send方法发送消息（具体实现是NetworkClient）</li><li>NetworkClient调用Selector.send</li><li>Selector创建KafkaChanel，并将请求写入通道</li><li>Sender调用KafkaClient.poll方法触发KafkaChanel真正执行发送，并执行回调方法</li></ol></blockquote><h1 id="生产者拦截器"><a class="markdownIt-Anchor" href="#生产者拦截器"></a> 生产者拦截器</h1><p>生产者拦截器既可以用来在消息发送前做一些准备工作， 比如按照某个规则过滤不符合要 求的消息、修改消息的内容等， 也可以用来在发送回调逻辑前做一些定制化的需求，比如统计 类工作。</p><p>使用生产者拦截器，只需要实现Producerlnterceptor，然后配置<code>interceptor.classes </code>即可，包含三个方法：</p><ol><li>onSend()方法：在将消息序列化和计算分区之前会调用，对消息进行相应 的定制化操作；</li><li>onAcknowledgement()方法：在消息被应答( Acknowledgement)之前或消息发送失败时调用，在callback之前。这个方法在producer的I/O线程中，所以逻辑应该尽量简单，否则会影响消息的发送。</li><li>close()方法：在关闭拦截器时执行一些资源的清理工作。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ProducerInterceptor</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Configurable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> ProducerRecord&lt;K, V&gt; <span class="title function_">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="重要的消费者参数"><a class="markdownIt-Anchor" href="#重要的消费者参数"></a> 重要的消费者参数</h1><ol><li><p>acks</p><blockquote><ul><li>acks=1。默认值。生产者需要收到服务端的响应才算发送消息成功。<em>如果服务端leader收到数据，但是follower还没有同步数据，此时leader副本崩溃，会丢失消息。如果发生leader选举，会返回一个错误消息。</em></li><li>acks=0。生产者发送消息之后不需要等待服务端响应。</li><li>acks=-1或acks=all。生产者发送消息之后，需要等待ISR中所有副本都成功写入消息之后，才能收到服务端的成功响应。<em>ISR中只有一个副本时，还是会丢失消息</em></li></ul></blockquote></li><li><p>max.request.size</p><blockquote><p>生产者客户端能发送消息的最大值，默认1MB。修改这个参数的时候还需要修改broker<code>message.max .bytes</code>参数，比如生产者的<code>max.request.size</code>配置成20，但是broker的<code>message.max .bytes</code>配置成10，此时发送了一个15B的消息，服务端就接收不了。</p></blockquote></li><li><p><a href="http://xn--retriesretry-4l2u.backoff.ms">retries和retry.backoff.ms</a></p><blockquote><ul><li><code>retries</code>用来配置生产者的重试次数，默认为0，即发生异常时不重试。如果重试的次数超过配置的次数，仍然失败，就会返回异常</li><li><code>retry.backoff.ms</code>用来配置两次重试之间的时间间隔，默认为100。</li></ul></blockquote><p>Kafka的同一个topic中的消息时有序的，生产者会按照发送的顺序发送给服务端，消费者也可以按照同样的顺序进行消费。如果配置了重试，而且配置的发送消息的并发数大于1（max.in.flight.requests.per .connection），此时第一批消息写入失败，而第二批消息写入成功，就会导致消息的顺序不一致。</p></li><li><p>batch.size</p><blockquote><p>生产者客户端发送消息，一个批次的大小，一个批次满了以后，就会发送到服务端。</p></blockquote></li><li><p><a href="http://linger.ms">linger.ms</a></p><blockquote><p>生产者客户端等待发送一批消息的最长时间，默认为0。即有消息就会发送到服务端。</p></blockquote></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;Kafka的生产者发送数据时，先将数据缓存到记录收集器RecordAccumulator中，再由发送线程Sender将消息批量地发送给服务端。&lt;/p&gt;</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="kafka" scheme="https://eoccc.gitee.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>深拷贝和浅拷贝工具</title>
    <link href="https://eoccc.gitee.io/2022/06/14/%E6%B7%B1%E6%8B%B7%E8%B4%9D%E5%92%8C%E6%B5%85%E6%8B%B7%E8%B4%9D%E5%B7%A5%E5%85%B7/"/>
    <id>https://eoccc.gitee.io/2022/06/14/%E6%B7%B1%E6%8B%B7%E8%B4%9D%E5%92%8C%E6%B5%85%E6%8B%B7%E8%B4%9D%E5%B7%A5%E5%85%B7/</id>
    <published>2022-06-14T07:10:35.000Z</published>
    <updated>2022-08-01T04:58:42.278Z</updated>
    
    <content type="html"><![CDATA[<p>前几天写代码的时候，需要拷贝对象，就使用了<code>org.apache.commons.beanutils.BeanUtils</code>的<code>BeanUtils.copyProperties(Object dest, Object orig)</code>拷贝对象，后面又修改了新对象的属性，就导致原对象也被修改了，仔细一研究才发现这个工具只是进行了浅拷贝。索性整理一下现在比较常用的一些深拷贝和浅拷贝工具。</p><span id="more"></span><h2 id="深拷贝"><a class="markdownIt-Anchor" href="#深拷贝"></a> 深拷贝</h2><h3 id="1-orika的mapperfactory"><a class="markdownIt-Anchor" href="#1-orika的mapperfactory"></a> 1. Orika的MapperFactory</h3><p>Orika底层采用了javassist类库生成Bean映射的字节码，之后直接加载执行生成的字节码文件，因此在速度上比使用反射进行赋值会快很多。<strong>线程安全，可以使用单例。推荐！</strong></p><p>首先引入依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ma.glasnost.orika<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>orika-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可以拷贝单个对象，也可以拷贝列表，这里只介绍单个对象的拷贝：</p><ul><li><p>直接克隆对象</p><p>克隆的对象可以不同，深拷贝两个对象相同的属性，跳过不同的属性。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">orikaClone</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">MapperFactory</span> <span class="variable">mapperFactory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DefaultMapperFactory</span>.Builder().build();</span><br><span class="line">  <span class="type">MapperFacade</span> <span class="variable">mapperFacade</span> <span class="operator">=</span> mapperFactory.getMapperFacade();</span><br><span class="line"></span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">fromObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  fromObj.setStr(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">  fromObj.setList(Lists.newArrayList(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>));</span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">toObj</span> <span class="operator">=</span> mapperFacade.map(fromObj, DemoObj.class);</span><br><span class="line">  toObj.setStr(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">  toObj.getList().add(<span class="string">&quot;C&quot;</span>);</span><br><span class="line">  System.out.println(JSON.toJSONString(fromObj));</span><br><span class="line">  System.out.println(JSON.toJSONString(toObj));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;],&quot;str&quot;:&quot;a&quot;&#125;</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;b&quot;&#125;</span></span><br></pre></td></tr></table></figure></li><li><p>拷贝对象属性</p><p>克隆的对象可以不同，深拷贝两个对象相同的属性，跳过不同的属性。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">orikaCopy</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">MapperFactory</span> <span class="variable">mapperFactory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DefaultMapperFactory</span>.Builder().build();</span><br><span class="line">  <span class="type">MapperFacade</span> <span class="variable">mapperFacade</span> <span class="operator">=</span> mapperFactory.getMapperFacade();</span><br><span class="line"></span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">fromObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  fromObj.setStr(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">  fromObj.setList(Lists.newArrayList(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>));</span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">toObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  mapperFacade.map(fromObj, toObj);</span><br><span class="line">  toObj.setStr(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">  toObj.getList().add(<span class="string">&quot;C&quot;</span>);</span><br><span class="line">  System.out.println(JSON.toJSONString(fromObj));</span><br><span class="line">  System.out.println(JSON.toJSONString(toObj));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;],&quot;str&quot;:&quot;a&quot;&#125;</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;b&quot;&#125;</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="dozer的dozerbeanmapper"><a class="markdownIt-Anchor" href="#dozer的dozerbeanmapper"></a> Dozer的DozerBeanMapper</h3><p>dozer是一种JavaBean的映射工具，类似于apache的BeanUtils。但是dozer更强大，它可以灵活的处理复杂类型之间的映射。不但可以进行简单的属性映射、复杂的类型映射、双向映射、递归映射等，并且可以通过XML配置文件进行灵活的配置。 <strong>线程安全，可以使用单例，性能一般。</strong></p><p>引入依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.sf.dozer<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>dozer<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.5.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>深拷贝属性，对象可以不同，属性不同时跳过。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">dozerCopy</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">DozerBeanMapper</span> <span class="variable">dozer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DozerBeanMapper</span>();</span><br><span class="line"></span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">fromObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  fromObj.setStr(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">  fromObj.setList(Lists.newArrayList(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>));</span><br><span class="line">  <span class="type">DemoObj2</span> <span class="variable">toObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj2</span>();</span><br><span class="line">  dozer.map(fromObj, toObj);</span><br><span class="line">  toObj.setStr(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">  toObj.getList().add(<span class="string">&quot;C&quot;</span>);</span><br><span class="line">  System.out.println(JSON.toJSONString(fromObj));</span><br><span class="line">  System.out.println(JSON.toJSONString(toObj));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;],&quot;str&quot;:&quot;a&quot;&#125;</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;b&quot;&#125;</span></span><br></pre></td></tr></table></figure><h3 id="利用序列化实现深拷贝"><a class="markdownIt-Anchor" href="#利用序列化实现深拷贝"></a> 利用序列化实现深拷贝</h3><p>利用输入输出流，将旧对象写入到新对象，实现拷贝。<strong>性能低，不推荐。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">DeepCloneDemo</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line">  String str;</span><br><span class="line">  List&lt;String&gt; list;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> DeepCloneDemo <span class="title function_">deepClone</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">DeepCloneDemo</span> <span class="variable">to</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="type">DeepCloneDemo</span> <span class="variable">from</span> <span class="operator">=</span> <span class="built_in">this</span>;</span><br><span class="line">    <span class="type">PipedOutputStream</span> <span class="variable">out</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PipedOutputStream</span>();</span><br><span class="line">    <span class="type">PipedInputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PipedInputStream</span>();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      in.connect(out);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> (<span class="type">ObjectOutputStream</span> <span class="variable">bo</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ObjectOutputStream</span>(out);</span><br><span class="line">         <span class="type">ObjectInputStream</span> <span class="variable">bi</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ObjectInputStream</span>(in);) &#123;</span><br><span class="line">      bo.writeObject(from);</span><br><span class="line">      to = (DeepCloneDemo) bi.readObject();</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> to;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">deepClone</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">DeepCloneDemo</span> <span class="variable">from</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DeepCloneDemo</span>();</span><br><span class="line">  from.setStr(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">  from.setList(Lists.newArrayList(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>));</span><br><span class="line">  <span class="type">DeepCloneDemo</span> <span class="variable">to</span> <span class="operator">=</span> from.deepClone();</span><br><span class="line">  to.setStr(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">  to.getList().add(<span class="string">&quot;C&quot;</span>);</span><br><span class="line">  System.out.println(JSON.toJSONString(from));</span><br><span class="line">  System.out.println(JSON.toJSONString(to));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;],&quot;str&quot;:&quot;a&quot;&#125;</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;b&quot;&#125;</span></span><br></pre></td></tr></table></figure><h2 id="浅拷贝"><a class="markdownIt-Anchor" href="#浅拷贝"></a> 浅拷贝</h2><h3 id="1-apache的beanutils"><a class="markdownIt-Anchor" href="#1-apache的beanutils"></a> 1. apache的BeanUtils</h3><p>所处的包：<code>org.apache.commons.beanutils.BeanUtils</code></p><p>基于反射拷贝，提供了两个拷贝对象的方法：</p><ul><li><p><code>BeanUtils.cloneBean(final Object bean)</code></p><p>传入一个对象，浅拷贝生成一个新的对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apacheClone</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">fromObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  fromObj.setS(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">  fromObj.setL(Lists.newArrayList(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>));</span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">toObj</span> <span class="operator">=</span> (DemoObj) org.apache.commons.beanutils.BeanUtils.cloneBean(fromObj);</span><br><span class="line">  toObj.setStr(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">  toObj.getList().add(<span class="string">&quot;C&quot;</span>);</span><br><span class="line">  System.out.println(JSON.toJSONString(fromObj));</span><br><span class="line">  System.out.println(JSON.toJSONString(toObj));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;a&quot;&#125;</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;b&quot;&#125;</span></span><br></pre></td></tr></table></figure></li><li><p><code>BeanUtils.copyProperties(final Object dest, final Object orig)</code></p><p>传入两个对象，浅拷贝相同的属性的值。两个对象的类型可以不一样，属性及属性类型相同即可，忽略不同的属性。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apacheCopy</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">fromObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  fromObj.setS(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">  fromObj.setL(Lists.newArrayList(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>));</span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">toObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  org.apache.commons.beanutils.BeanUtils.copyProperties(toObj, fromObj);</span><br><span class="line">  toObj.setStr(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">  toObj.getList().add(<span class="string">&quot;C&quot;</span>);</span><br><span class="line">  System.out.println(JSON.toJSONString(fromObj));</span><br><span class="line">  System.out.println(JSON.toJSONString(toObj));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;a&quot;&#125;</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;b&quot;&#125;</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="2-springframework的beanutils"><a class="markdownIt-Anchor" href="#2-springframework的beanutils"></a> 2. springframework的BeanUtils</h3><p>所处的包：<code>org.springframework.beans.BeanUtils</code></p><p>提供了四个拷贝对象的方法，基本原理都一样，只不过是做了一些定制。这里只整理基本的拷贝方法：</p><ul><li><p><code>BeanUtils.copyProperties(Object source, Object target)</code></p><p>功能与apache的相似，需要注意的是，传入对象的顺序刚好相反，原对象在第一个，新对象在第二个</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">springCopy</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">fromObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  fromObj.setStr(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">  fromObj.setList(Lists.newArrayList(<span class="string">&quot;A&quot;</span>, <span class="string">&quot;B&quot;</span>));</span><br><span class="line">  <span class="type">DemoObj</span> <span class="variable">toObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">DemoObj</span>();</span><br><span class="line">  org.springframework.beans.BeanUtils.copyProperties(fromObj, toObj);</span><br><span class="line">  toObj.setStr(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">  toObj.getList().add(<span class="string">&quot;C&quot;</span>);</span><br><span class="line">  System.out.println(JSON.toJSONString(fromObj));</span><br><span class="line">  System.out.println(JSON.toJSONString(toObj));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;a&quot;&#125;</span></span><br><span class="line"><span class="comment">//&#123;&quot;list&quot;:[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;],&quot;str&quot;:&quot;b&quot;&#125;</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;前几天写代码的时候，需要拷贝对象，就使用了&lt;code&gt;org.apache.commons.beanutils.BeanUtils&lt;/code&gt;的&lt;code&gt;BeanUtils.copyProperties(Object dest, Object orig)&lt;/code&gt;拷贝对象，后面又修改了新对象的属性，就导致原对象也被修改了，仔细一研究才发现这个工具只是进行了浅拷贝。索性整理一下现在比较常用的一些深拷贝和浅拷贝工具。&lt;/p&gt;</summary>
    
    
    
    
    <category term="技术笔记" scheme="https://eoccc.gitee.io/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"/>
    
    <category term="踩坑" scheme="https://eoccc.gitee.io/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>dubbo优雅停机</title>
    <link href="https://eoccc.gitee.io/2022/05/14/dubbo%E4%BC%98%E9%9B%85%E5%81%9C%E6%9C%BA/"/>
    <id>https://eoccc.gitee.io/2022/05/14/dubbo%E4%BC%98%E9%9B%85%E5%81%9C%E6%9C%BA/</id>
    <published>2022-05-13T16:00:00.000Z</published>
    <updated>2022-08-01T04:58:42.247Z</updated>
    
    <content type="html"><![CDATA[<p>优雅停机是Dubbo的重要特性之一，因为核心业务运行时突然中断可能带来严重的后果。</p><span id="more"></span><p>Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 <code>kill -9 PID</code> 等强制关闭指令，是不会执行优雅停机的，只有通过 <code>kill PID</code> 时，才会执行。</p><p>Dubbo收到停机的指令后，会进行如下操作：</p><blockquote><ol><li>dubbo收到退出指令</li><li>provider端取消注册中心的元数据</li><li>注册中心将元数据的变更推送给consumer端</li><li>consumer端更新元数据（会移除准备停机的provider元数据）</li><li>providre端发送readonly报文通知consumer端服务不可用</li><li>providre端拒绝新的任务，等待正在执行的任务完成后断开连接</li></ol></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3qf3zjlwjj21620igmz4.jpg" alt="image-20220630174211346" style="zoom:40%;"><p>第4步provider端主动发送报文同志consumer端服务下线，是为了避免第3步注册中心通知consumer端导致网络延迟。</p><p><code>DubboShutdownHook</code>在启动dubbo的时候注册到RunTime的hooks中去，源码位于<code>org.apache.dubbo.config.deploy.DefaultApplicationDeployer#registerShutdownHook</code>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">registerShutdownHook</span><span class="params">()</span> &#123;</span><br><span class="line">dubboShutdownHook.register();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//往下跟dubboShutdownHook.register()</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">register</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (registered.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>) &amp;&amp; !ignoreListenShutdownHook) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      Runtime.getRuntime().addShutdownHook(<span class="built_in">this</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalStateException e) &#123;</span><br><span class="line">      logger.warn(<span class="string">&quot;register shutdown hook failed: &quot;</span> + e.getMessage());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      logger.warn(<span class="string">&quot;register shutdown hook failed: &quot;</span> + e.getMessage(), e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//继续跟 Runtime.getRuntime().addShutdownHook</span></span><br><span class="line"><span class="comment">//这就到了java层了</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">addShutdownHook</span><span class="params">(Thread hook)</span> &#123;</span><br><span class="line">  <span class="type">SecurityManager</span> <span class="variable">sm</span> <span class="operator">=</span> System.getSecurityManager();</span><br><span class="line">  <span class="keyword">if</span> (sm != <span class="literal">null</span>) &#123;</span><br><span class="line">    sm.checkPermission(<span class="keyword">new</span> <span class="title class_">RuntimePermission</span>(<span class="string">&quot;shutdownHooks&quot;</span>));</span><br><span class="line">  &#125;</span><br><span class="line">  ApplicationShutdownHooks.add(hook);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//继续跟 ApplicationShutdownHooks.add</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(Thread hook)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span>(hooks == <span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalStateException</span>(<span class="string">&quot;Shutdown in progress&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (hook.isAlive())</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>(<span class="string">&quot;Hook already running&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (hooks.containsKey(hook))</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>(<span class="string">&quot;Hook previously registered&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//塞到了hooks这个map中</span></span><br><span class="line">  hooks.put(hook, hook);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//调用钩子：java.lang.ApplicationShutdownHooks#runHooks</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">runHooks</span><span class="params">()</span> &#123;</span><br><span class="line">  Collection&lt;Thread&gt; threads;</span><br><span class="line">  <span class="keyword">synchronized</span>(ApplicationShutdownHooks.class) &#123;</span><br><span class="line">    threads = hooks.keySet();</span><br><span class="line">    hooks = <span class="literal">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (Thread hook : threads) &#123;</span><br><span class="line">    hook.start();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (Thread hook : threads) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      hook.join();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException x) &#123; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//最后调用钩子的入口是：java.lang.Runtime#exit</span></span><br></pre></td></tr></table></figure><p><code>DubboShutdownHook</code>都干了些啥呢？核心方法就一个<code>destroy()</code>，其中最重要的是<code>onDestroy()</code>，后续的大致就是移除了一些类加载器、spring容器、扩展类加载器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">destroy</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (destroyed.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">//销毁前的操作，非常核心！</span></span><br><span class="line">      onDestroy();</span><br><span class="line">      </span><br><span class="line">      <span class="comment">//后面都是一些销毁容器的操作</span></span><br><span class="line">      HashSet&lt;ClassLoader&gt; copyOfClassLoaders = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;(classLoaders);</span><br><span class="line">      <span class="keyword">for</span> (ClassLoader classLoader : copyOfClassLoaders) &#123;</span><br><span class="line">        removeClassLoader(classLoader);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (beanFactory != <span class="literal">null</span>) &#123;</span><br><span class="line">        beanFactory.destroy();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (extensionDirector != <span class="literal">null</span>) &#123;</span><br><span class="line">        extensionDirector.destroy();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">      LOGGER.error(<span class="string">&quot;Error happened when destroying ScopeModel.&quot;</span>, t);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重点看下<code>org.apache.dubbo.rpc.model.ApplicationModel#onDestroy</code>方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">onDestroy</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="comment">// 1. remove from frameworkModel</span></span><br><span class="line">  frameworkModel.removeApplication(<span class="built_in">this</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. pre-destroy, set stopping</span></span><br><span class="line">  <span class="keyword">if</span> (deployer != <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="comment">// destroy registries and unregister services from registries first to notify consumers to stop consuming this instance.</span></span><br><span class="line">    <span class="comment">// 移除注册中心的元数据，以通过注册中心通知consumer停止调用服务，包括销毁服务发现、元数据等</span></span><br><span class="line">    deployer.preDestroy();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. Try to destroy protocols to stop this instance from receiving new requests from connections</span></span><br><span class="line">  <span class="comment">// 销毁协议，从而停止接收新的请求</span></span><br><span class="line">  frameworkModel.tryDestroyProtocols();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4. destroy application resources</span></span><br><span class="line">  <span class="keyword">for</span> (ModuleModel moduleModel : <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(moduleModels)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (moduleModel != internalModule) &#123;</span><br><span class="line">      moduleModel.destroy();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 5. destroy internal module later</span></span><br><span class="line">  internalModule.destroy();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6. post-destroy, release registry resources</span></span><br><span class="line">  <span class="keyword">if</span> (deployer != <span class="literal">null</span>) &#123;</span><br><span class="line">    deployer.postDestroy();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 7. destroy other resources (e.g. ZookeeperTransporter )</span></span><br><span class="line">  notifyDestroy();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (environment != <span class="literal">null</span>) &#123;</span><br><span class="line">    environment.destroy();</span><br><span class="line">    environment = <span class="literal">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (configManager != <span class="literal">null</span>) &#123;</span><br><span class="line">    configManager.destroy();</span><br><span class="line">    configManager = <span class="literal">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (serviceRepository != <span class="literal">null</span>) &#123;</span><br><span class="line">    serviceRepository.destroy();</span><br><span class="line">    serviceRepository = <span class="literal">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 8. destroy framework if none application</span></span><br><span class="line">  frameworkModel.tryDestroy();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;优雅停机是Dubbo的重要特性之一，因为核心业务运行时突然中断可能带来严重的后果。&lt;/p&gt;</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="dubbo" scheme="https://eoccc.gitee.io/tags/dubbo/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出对比数组和链表</title>
    <link href="https://eoccc.gitee.io/2022/05/07/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AF%B9%E6%AF%94%E6%95%B0%E7%BB%84%E5%92%8C%E9%93%BE%E8%A1%A8/"/>
    <id>https://eoccc.gitee.io/2022/05/07/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AF%B9%E6%AF%94%E6%95%B0%E7%BB%84%E5%92%8C%E9%93%BE%E8%A1%A8/</id>
    <published>2022-05-07T08:25:19.000Z</published>
    <updated>2022-08-01T04:58:42.278Z</updated>
    
    <content type="html"><![CDATA[<p>数组和链表是我们日常编程中非常常用的两种数据结构，那么它们的性能表现如何，本文由浅入深对此进行一些分析。</p><span id="more"></span><h2 id="内存分配"><a class="markdownIt-Anchor" href="#内存分配"></a> 内存分配</h2><p>在开始分析之前，我们先来了解一下数组和链表在内存中的存储方式。</p><p>数组在内存中是存储在一个连续的空间。对于基础数据类型，数组的元素存储的是数据本身，对于引用类型，元素存储的是引用地址（一个Integer值）。在为数组由两部分组成：数组的元数据（16字节）、数组的元素（基本数据类型为元素本身大小，引用类型为引用地址–8个字节）。</p><p>链表在内存中是分布在非连续的空间中的，通过地址指针指向下一个或上一个节点。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h35mtu68wqj20qy0hi403.jpg" alt="image-20220610090434150" style="zoom:50%;"><p>在64位jvm中，内存页为内存分配的最小单位，每个内存页的大小为8个字节。因此，在为对象分配内存的时候，必须为8的倍数，如果元素占用的内存小于8个字节，也会分配8个字节。如：</p><blockquote><ol><li><code>new int[1]</code>的大小为24个字节，即元素int的4个字节 + 4个空白字节（因为一个内存页为8个字节，） + 数组内部属性16个字节；</li><li><code>new long[1]</code>的大小为24个字节，即元素long的8个字节 + 数组内部属性16个字节；</li><li><code>new Node[1]</code>将会分配24个字节，即引用地址Integer的4个字节 + 4个空白字节 + 数组内部属性16个字节</li></ol></blockquote><p>接下来我们进行测试：我们分别对boolean、int、long、Node对象新建容量从1～10的数组，然后分别统计每个数组占用内存的大小。</p><p>这里我使用jol的ClassLayout工具获取对象占用的内存大小，引入依赖即可使用：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.openjdk.jol<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jol-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>测试代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test</span><span class="params">()</span> <span class="keyword">throws</span> ParseException &#123;</span><br><span class="line">  System.out.println(<span class="string">&quot;capacity booleanArraySize intArraySize longArraySize nodeArraySize&quot;</span>);</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="type">boolean</span>[] booleanArray = <span class="keyword">new</span> <span class="title class_">boolean</span>[i];</span><br><span class="line">    <span class="type">int</span>[] intArray = <span class="keyword">new</span> <span class="title class_">int</span>[i];</span><br><span class="line">    <span class="type">long</span>[] longArray = <span class="keyword">new</span> <span class="title class_">long</span>[i];</span><br><span class="line">    Node[] nodeArray = <span class="keyword">new</span> <span class="title class_">Node</span>[i];</span><br><span class="line">    <span class="type">long</span> <span class="variable">booleanArraySize</span> <span class="operator">=</span> ClassLayout.parseInstance(booleanArray).instanceSize();</span><br><span class="line">    <span class="type">long</span> <span class="variable">intArraySize</span> <span class="operator">=</span> ClassLayout.parseInstance(intArray).instanceSize();</span><br><span class="line">    <span class="type">long</span> <span class="variable">longArraySize</span> <span class="operator">=</span> ClassLayout.parseInstance(longArray).instanceSize();</span><br><span class="line">    <span class="type">long</span> <span class="variable">nodeArraySize</span> <span class="operator">=</span> ClassLayout.parseInstance(nodeArray).instanceSize();</span><br><span class="line">    System.out.printf(<span class="string">&quot;%d %d %d %d %d%n&quot;</span>, i, booleanArraySize, intArraySize, longArraySize, nodeArraySize);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Node</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> Node pre;</span><br><span class="line">  <span class="keyword">private</span> Node next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>统计的结果如下表所示。如boolean数组，初始大小为24字节，除去数组自身的16字节以外，还有8个字节可以用来保存元素，boolean占用1个字节，也就是说还可以存8个boolean元素，正如实验结果一样，数组容量为1～8的时候，占用内存为24字节，当数组容量为9的时候变成32字节（24+8，即增加了一个内存页）。</p><table><thead><tr><th>capacity</th><th>booleanArraySize</th><th>intArraySize</th><th>longArraySize</th><th>nodeArraySize</th></tr></thead><tbody><tr><td>1</td><td>24</td><td>24</td><td>24</td><td>24</td></tr><tr><td>2</td><td>24</td><td>24</td><td>32</td><td>24</td></tr><tr><td>3</td><td>24</td><td>32</td><td>40</td><td>32</td></tr><tr><td>4</td><td>24</td><td>32</td><td>48</td><td>32</td></tr><tr><td>5</td><td>24</td><td>40</td><td>56</td><td>40</td></tr><tr><td>6</td><td>24</td><td>40</td><td>64</td><td>40</td></tr><tr><td>7</td><td>24</td><td>48</td><td>72</td><td>48</td></tr><tr><td>8</td><td>24</td><td>48</td><td>80</td><td>48</td></tr><tr><td>9</td><td>32</td><td>56</td><td>88</td><td>56</td></tr><tr><td>10</td><td>32</td><td>56</td><td>96</td><td>56</td></tr></tbody></table><h2 id="添加"><a class="markdownIt-Anchor" href="#添加"></a> 添加</h2><p><strong>数组插入</strong></p><p>对于数组来说，在末尾添加一个元素的流程大致是这样的：</p><blockquote><ol><li>找到数组在内存中的起始地址；</li><li>根据下标计算出需要插入元素的位置的地址；</li><li>如果是基本数据类型，直接将元素写入到内存中；</li><li>如果是引用类型，将元素的地址写入到需要添加元素的内存地址</li></ol></blockquote><p>如下图是一个容量为3的int数组的示例，数组中已经保存了两个元素，由于是基本类型，没有赋值的元素（即第三个元素）会初始化为0。数组在内存中的起始地址为1000，metadate占16字节，因此第一个元素的起始位置是1016，由于内存页为8个字节，保存3个元素需要两个内存页，因此数组的结束位置为1032。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h35mtp6f6ej20iy066q32.jpg" alt="image-20220609103236268" style="zoom:67%;"><p>如果要在第三个位置添加一个元素，即如下操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array[<span class="number">2</span>] = <span class="number">3</span>;</span><br></pre></td></tr></table></figure><p>那么会进行如下操作：</p><blockquote><ol><li>找到数组在内存中的位置1000</li><li>计算出需要插入的元素的位置：1000 + 16 + 4 * 2 = 1024</li><li>判断地址是否超出数组分配的内存地址范围，超出则说明数组越界，抛出<em>IndexOutOfBoundsException</em></li><li>将需要插入的值写入到指定的内存位置</li></ol></blockquote><p><strong>链表添加元素</strong></p><p>链表添加元素就比较简单，直接把Node的引用地址赋值给末尾Node的next即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail.next = <span class="keyword">new</span> <span class="title class_">Node</span>();</span><br></pre></td></tr></table></figure><p><strong>性能分析</strong></p><p>数组追加元素比链表追加元素要多两个操作：计算元素的位置、判断是否越界。因此我猜测链表添加元素的速度要快一些。测试一下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testAdd</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">maxCount</span> <span class="operator">=</span> <span class="number">1000000</span>;</span><br><span class="line">    System.out.println(<span class="string">&quot;count    array add cost&quot;</span>);</span><br><span class="line">    <span class="keyword">while</span> (count &lt; maxCount) &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.nanoTime();</span><br><span class="line">        arrayAdd(count);</span><br><span class="line">        System.out.println(count + <span class="string">&quot;    &quot;</span> + (System.nanoTime() - start));</span><br><span class="line">        count = count &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    System.out.println(<span class="string">&quot;count    link add cost&quot;</span>);</span><br><span class="line">    count = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">while</span> (count &lt; maxCount) &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.nanoTime();</span><br><span class="line">        linkAdd(count);</span><br><span class="line">        System.out.println(count + <span class="string">&quot;    &quot;</span> + (System.nanoTime() - start));</span><br><span class="line">        count = count &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Node[] arrayAdd(<span class="type">int</span> count) &#123;</span><br><span class="line">    Node[] nodes = <span class="keyword">new</span> <span class="title class_">Node</span>[count];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; count - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        nodes[i] = <span class="keyword">new</span> <span class="title class_">Node</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> nodes;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Node <span class="title function_">linkAdd</span><span class="params">(<span class="type">int</span> count)</span> &#123;</span><br><span class="line">    <span class="type">Node</span> <span class="variable">head</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Node</span>();</span><br><span class="line">    <span class="type">Node</span> <span class="variable">tmp</span> <span class="operator">=</span> head;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; count - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        tmp.next = <span class="keyword">new</span> <span class="title class_">Node</span>();</span><br><span class="line">        tmp = tmp.next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分析一下数据，如下图，横坐标是元素的数量，纵坐标是耗时（纳秒）。可以看到，在一定数量范围内，数组追加元素的速度比链表要快一些，但是超过一定的数据量后，链表要快一些。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h35mtm8h8qj20qq0g2jss.jpg" alt="image-20220610092601983" style="zoom:50%;"><p>图中add的耗时存在一个先增后降的过程，这里可能涉及到jvm和操作系统更底层的实现逻辑了，暂时没有分析出原因。</p><h2 id="插入"><a class="markdownIt-Anchor" href="#插入"></a> 插入</h2><p><strong>数组插入</strong></p><p>数组的插入是一个比较重的操作，大致流程如下：</p><blockquote><ol><li>根据下标定位到需要插入的内存地址（通过偏移量计算）</li><li>将后面的元素往后移动一个位置（涉及到大块的内存移动）</li><li>将新的元素写到指定的位置</li></ol></blockquote><p>数组插入代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="title function_">arrayInsert</span><span class="params">(<span class="type">int</span> index, <span class="type">int</span> currSize, Node[] nodes, Node node)</span> &#123;</span><br><span class="line">    <span class="comment">//先把后面的元素后移一位</span></span><br><span class="line">    <span class="keyword">while</span> (currSize &gt; index) &#123;</span><br><span class="line">        nodes[currSize] = nodes[currSize - <span class="number">1</span>];</span><br><span class="line">        currSize--;</span><br><span class="line">    &#125;</span><br><span class="line">    nodes[index] = node;</span><br><span class="line">    <span class="keyword">return</span> currSize+<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>链表插入</strong></p><p>链表的插入比较简单，只需要修改链表的指针即可，而且只需要修改2个节点的指针（双向链表需要修改3个节点的指针）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//单向链表插入</span></span><br><span class="line"><span class="type">Node</span> <span class="variable">next</span> <span class="operator">=</span> pre.next;</span><br><span class="line">pre.next = node;</span><br><span class="line">node.next = next;</span><br></pre></td></tr></table></figure><p>但是如果我们只知道要插入到第几个，还得先通过遍历定位出插入节点的位置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">linkInsert</span><span class="params">(<span class="type">int</span> index, Node head, Node node)</span> &#123;</span><br><span class="line">    <span class="type">Node</span> <span class="variable">tmp</span> <span class="operator">=</span> head;</span><br><span class="line">    <span class="comment">//先找到插入的位置</span></span><br><span class="line">    <span class="keyword">while</span> (index &gt; <span class="number">0</span> &amp;&amp; tmp != <span class="literal">null</span>) &#123;</span><br><span class="line">        tmp = tmp.next;</span><br><span class="line">        index--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//越界了</span></span><br><span class="line">    <span class="keyword">if</span> (index &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Node</span> <span class="variable">next</span> <span class="operator">=</span> tmp.next;</span><br><span class="line">    tmp.next = node;</span><br><span class="line">    node.next = next;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>性能对比</strong></p><p>链表不涉及内存的移动，只需要定位到插入的位置即可，复杂度为O(index)，因此我猜测链表的插入速度要快一些。我们测试在size/2的位置插入元素：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testInsert</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">maxSize</span> <span class="operator">=</span> <span class="number">1000000</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">currSize</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">Node</span> <span class="variable">head</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Node</span>();</span><br><span class="line">    Node[] nodes = <span class="keyword">new</span> <span class="title class_">Node</span>[maxSize];</span><br><span class="line">    nodes[<span class="number">0</span>] = head;</span><br><span class="line">    <span class="type">Node</span> <span class="variable">tail</span> <span class="operator">=</span> head;</span><br><span class="line">    tail = addElement(nodes, tail, <span class="number">0</span>, size);</span><br><span class="line"></span><br><span class="line">    System.out.println(<span class="string">&quot;count    array insert cost    link insert cost&quot;</span>);</span><br><span class="line">    <span class="keyword">while</span> (size &lt; maxSize) &#123;</span><br><span class="line">        <span class="type">Node</span> <span class="variable">node</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Node</span>();</span><br><span class="line">        <span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> size &gt;&gt; <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.nanoTime();</span><br><span class="line">        currSize = arrayInsert(index, currSize, nodes, node);</span><br><span class="line">        System.out.print(size + <span class="string">&quot;    &quot;</span> + (System.nanoTime() - start));</span><br><span class="line"></span><br><span class="line">        start = System.nanoTime();</span><br><span class="line">        linkInsert(index, head, node);</span><br><span class="line">        System.out.println(<span class="string">&quot;    &quot;</span> + (System.nanoTime() - start));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//添加元素</span></span><br><span class="line">        size = size &lt;&lt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (size &lt; maxSize) &#123;</span><br><span class="line">            tail = addElement(nodes, tail, currSize, size);</span><br><span class="line">            currSize++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果：</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h35mu1a41yj20qr0g2759.jpg" alt="image-20220611150741041" style="zoom:50%;"><h2 id="删除"><a class="markdownIt-Anchor" href="#删除"></a> 删除</h2><p>删除和插入类似，数组会导致内存块的移动，链表只需要修改节点的指针即可，这里不再赘述。</p><h2 id="遍历"><a class="markdownIt-Anchor" href="#遍历"></a> 遍历</h2><p>数组的遍历需要根据下标，计算出元素的地址，然后取出元素。链表的遍历依次获取元素的地址指针即可。但是由于数组在内存中是顺序保存的，操作系统加载数据的时候，会把缓存页中的数据全都加载到高速缓存，因此数据的遍历速度会比链表快。</p><p>上代码测试分析：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testFor</span><span class="params">()</span> &#123;</span><br><span class="line">    Map&lt;Integer, List&lt;Long&gt;&gt; arrayCostMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    Map&lt;Integer, List&lt;Long&gt;&gt; linkCostMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="type">int</span> <span class="variable">maxSize</span> <span class="operator">=</span> <span class="number">100000</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">currSize</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    <span class="comment">//为了削弱机器获胜虚拟机导致的误差，跑了10次取平均值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; <span class="number">10</span>; j++) &#123;</span><br><span class="line">        <span class="keyword">while</span> (currSize &lt; maxSize) &#123;</span><br><span class="line">            Node[] nodeArray = createArray(currSize);</span><br><span class="line">            <span class="type">Node</span> <span class="variable">head</span> <span class="operator">=</span> createLink(nodeArray);</span><br><span class="line"></span><br><span class="line">            <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.nanoTime();</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; currSize; i++) &#123;</span><br><span class="line">                <span class="type">Node</span> <span class="variable">node</span> <span class="operator">=</span> nodeArray[i];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">long</span> <span class="variable">arrayCost</span> <span class="operator">=</span> System.nanoTime() - start;</span><br><span class="line">            List&lt;Long&gt; arrayCostList = arrayCostMap.computeIfAbsent(currSize, k -&gt; <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;());</span><br><span class="line">            arrayCostList.add(arrayCost);</span><br><span class="line"></span><br><span class="line">            start = System.nanoTime();</span><br><span class="line">            <span class="type">Node</span> <span class="variable">tmp</span> <span class="operator">=</span> head;</span><br><span class="line">            <span class="keyword">while</span> (tmp != <span class="literal">null</span>) &#123;</span><br><span class="line">                tmp = tmp.next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">long</span> <span class="variable">linkCost</span> <span class="operator">=</span> System.nanoTime() - start;</span><br><span class="line">            List&lt;Long&gt; linkCostList = linkCostMap.computeIfAbsent(currSize, k -&gt; <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;());</span><br><span class="line">            linkCostList.add(linkCost);</span><br><span class="line">            currSize = currSize &lt;&lt; <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    currSize = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">while</span> (currSize &lt; maxSize) &#123;</span><br><span class="line">        <span class="type">Double</span> <span class="variable">arrayCost</span> <span class="operator">=</span> arrayCostMap.get(currSize).stream().collect(Collectors.averagingLong(Long::<span class="keyword">new</span>));</span><br><span class="line">        <span class="type">Double</span> <span class="variable">linkCost</span> <span class="operator">=</span> linkCostMap.get(currSize).stream().collect(Collectors.averagingLong(Long::<span class="keyword">new</span>));</span><br><span class="line">        System.out.printf(<span class="string">&quot;%d %s %s%n&quot;</span>, currSize, arrayCost, linkCost);</span><br><span class="line">        currSize = currSize &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Node[] createArray(<span class="type">int</span> size) &#123;</span><br><span class="line">    Node[] nodeArray = <span class="keyword">new</span> <span class="title class_">Node</span>[size];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        nodeArray[i] = <span class="keyword">new</span> <span class="title class_">Node</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> nodeArray;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Node <span class="title function_">createLink</span><span class="params">(Node[] nodeArray)</span> &#123;</span><br><span class="line">    <span class="type">Node</span> <span class="variable">head</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Node</span>();</span><br><span class="line">    <span class="type">Node</span> <span class="variable">tmp</span> <span class="operator">=</span> head;</span><br><span class="line">    <span class="keyword">for</span> (Node node : nodeArray) &#123;</span><br><span class="line">        tmp.next = node;</span><br><span class="line">        tmp = tmp.next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果分析如下图。整体来说，数组的遍历速度比链表快，尤其是在数据量比较大的时候，二者的差距越来越大。不过在前面数组和链表的遍历数组都有波动，猜测跟jvm、操作系统、硬件都有关系，感兴趣的可以和我一起研究。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h35mrm2w95j20k20c2t9b.jpg" alt="image-20220612181316593" style="zoom:50%;">]]></content>
    
    
    <summary type="html">&lt;p&gt;数组和链表是我们日常编程中非常常用的两种数据结构，那么它们的性能表现如何，本文由浅入深对此进行一些分析。&lt;/p&gt;</summary>
    
    
    
    
    <category term="随笔" scheme="https://eoccc.gitee.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>dubbo服务暴露</title>
    <link href="https://eoccc.gitee.io/2022/05/04/dubbo%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/"/>
    <id>https://eoccc.gitee.io/2022/05/04/dubbo%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2/</id>
    <published>2022-05-03T16:00:00.000Z</published>
    <updated>2022-08-01T04:58:42.247Z</updated>
    
    <content type="html"><![CDATA[<p>Dubbo的服务暴露主要分为两个部分：第一步，将服务实例通过代理转换成Invoker；第二步，将Invoker通过具体协议转换成Exporter，实现服务暴露。</p><span id="more"></span><p>Dubbo框架进行服务暴露的入口是ServiceConfig#export，暴露服务的流程大致分为以下几步：</p><blockquote><ol><li>读取配置。AbstractConfig#refresh</li><li>通过代理将服务实例转成invoker。默认是Javassist，也支持JDK动态代理。ServiceConfig#doExportUrl</li><li>通过具体的协议，将Invoker转成expoter</li><li>将服务的元数据注册到注册中心。ServiceConfig#exported</li></ol></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h3wc0ircowj20xo0rqwhl.jpg" style="zoom:33%;"><p>注意：</p><ol><li>在将服务的元数据推送到注册中心的时候，如果失败了会进行重试，默认重试次数为6次。</li><li>支持多注册中心。MetadataServiceNameMapping#map</li></ol><p><strong>配置文件优先级：</strong></p><blockquote><ol><li>-D 传递给 JVM 参数优先级最高，比如-Ddubbo. protocol.port=20880</li><li>代码或XML配置优先级次高，比如Spring中XML文件指定&lt;dubbo:protocol port=H20880’/&gt;</li><li>置文件优先级最低，比如 dubbo.properties 文件指定 dubbo.protocol.port=20880</li></ol></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;Dubbo的服务暴露主要分为两个部分：第一步，将服务实例通过代理转换成Invoker；第二步，将Invoker通过具体协议转换成Exporter，实现服务暴露。&lt;/p&gt;</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="dubbo" scheme="https://eoccc.gitee.io/tags/dubbo/"/>
    
  </entry>
  
  <entry>
    <title>dubbo服务消费</title>
    <link href="https://eoccc.gitee.io/2022/05/04/dubbo%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9/"/>
    <id>https://eoccc.gitee.io/2022/05/04/dubbo%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9/</id>
    <published>2022-05-03T16:00:00.000Z</published>
    <updated>2022-08-01T04:58:42.247Z</updated>
    
    <content type="html"><![CDATA[<p>在整体上看，Dubbo框架做服务消费也分为两大部分，第一步通过持有远程服务实例生成 Invoker，这个Invoker在客户端是核心的远程代理对象。第二步会把Invoker通过动态代理转换成实现用户接口的动态代理引用。这里的Invoker承载了网络连接、服务调用和重试等功能。</p><span id="more"></span><p>服务消费大致分为以下几步：</p><blockquote><ol><li><p>读取配置，转成ReferenceBean</p></li><li><p>获取元数据</p></li><li><p>创建invoker（incoker就是协议层包装了一个客户端）</p></li><li><p>代理invoker</p><p>支持cglib代理和jdk动态代理：CglibAopProxy、JdkDynamicAopProxy</p></li></ol></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h3yxnh5zhjj20rm0n441i.jpg" alt="image-20220707101619358" style="zoom:45%;"><p>Dubbo的消费者配置会被转换成ReferenceBean，获取dubbo代理的入口是ReferenceBean#getObject，如果没有初始化，会调用createLazyProxy()方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> T <span class="title function_">getObject</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (lazyProxy == <span class="literal">null</span>) &#123;</span><br><span class="line">    createLazyProxy();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> (T) lazyProxy;</span><br><span class="line">&#125;    </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">createLazyProxy</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="comment">//创建代理工厂</span></span><br><span class="line">  <span class="type">ProxyFactory</span> <span class="variable">proxyFactory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ProxyFactory</span>();</span><br><span class="line">  <span class="comment">//初始化代理的核心逻辑</span></span><br><span class="line">  proxyFactory.setTargetSource(<span class="keyword">new</span> <span class="title class_">DubboReferenceLazyInitTargetSource</span>());</span><br><span class="line">  ...</span><br><span class="line">  <span class="built_in">this</span>.lazyProxy = proxyFactory.getProxy(<span class="built_in">this</span>.beanClassLoader);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>DubboReferenceLazyInitTargetSource中的核心逻辑是调用了ReferenceConfig#get方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ReferenceBean#getCallProxy</span></span><br><span class="line"><span class="keyword">private</span> Object <span class="title function_">getCallProxy</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="keyword">if</span> (referenceConfig == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalStateException</span>(<span class="string">&quot;ReferenceBean is not ready yet, please make sure to call reference interface method after dubbo is started.&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//get reference proxy 核心！</span></span><br><span class="line">  <span class="keyword">return</span> referenceConfig.get();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在中，如果引用没有初始化，会先进行初始化，然后放到ReferenceCache中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> T <span class="title function_">get</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (destroyed) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalStateException</span>(<span class="string">&quot;The invoker of ReferenceConfig(&quot;</span> + url + <span class="string">&quot;) has already destroyed!&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (ref == <span class="literal">null</span>) &#123;</span><br><span class="line">    getScopeModel().getDeployer().start();</span><br><span class="line">    <span class="keyword">synchronized</span> (<span class="built_in">this</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (ref == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">//开始初始化</span></span><br><span class="line">        init();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ref;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ReferenceConfig#init开始初始化引用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">()</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 先初始化service的元数据</span></span><br><span class="line">  initServiceMetadata(consumer);</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 获取注册中心</span></span><br><span class="line">  <span class="type">ModuleServiceRepository</span> <span class="variable">repository</span> <span class="operator">=</span> getScopeModel().getServiceRepository();</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 将consumer注册到注册中心</span></span><br><span class="line">  repository.registerConsumer(consumerModel);</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 创建代理  重点！</span></span><br><span class="line">  ref = createProxy(referenceParameters);</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>createProxy方法中的重点是createInvokerForRemote，创建远程代理，实现了单注册中心和多注册中心的逻辑，另外createProxy也实现了本地代理和远程代理的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> T <span class="title function_">createProxy</span><span class="params">(Map&lt;String, String&gt; referenceParameters)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (shouldJvmRefer(referenceParameters)) &#123;</span><br><span class="line">    <span class="comment">//创建本地代理</span></span><br><span class="line">    createInvokerForLocal(referenceParameters);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">//创建远程代理</span></span><br><span class="line">    createInvokerForRemote();</span><br><span class="line">  &#125;</span><br><span class="line">...</span><br><span class="line">  <span class="comment">// create service proxy</span></span><br><span class="line">  <span class="keyword">return</span> (T) proxyFactory.getProxy(invoker, ProtocolUtils.isGeneric(generic));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">createInvokerForRemote</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="comment">// 单注册中心</span></span><br><span class="line">  <span class="keyword">if</span> (urls.size() == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="type">URL</span> <span class="variable">curUrl</span> <span class="operator">=</span> urls.get(<span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 核心 通过协议创建代理</span></span><br><span class="line">    invoker = protocolSPI.refer(interfaceClass, curUrl);</span><br><span class="line">    ...</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    List&lt;Invoker&lt;?&gt;&gt; invokers = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    <span class="type">URL</span> <span class="variable">registryUrl</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="comment">// 多注册中心</span></span><br><span class="line">    <span class="keyword">for</span> (URL url : urls) &#123;</span><br><span class="line">      <span class="comment">// 和单注册中心一样  创建代理</span></span><br><span class="line">      invokers.add(protocolSPI.refer(interfaceClass, url));</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>后续就是协议层的具体实现了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在整体上看，Dubbo框架做服务消费也分为两大部分，第一步通过持有远程服务实例生成 Invoker，这个Invoker在客户端是核心的远程代理对象。第二步会把Invoker通过动态代理转换成实现用户接口的动态代理引用。这里的Invoker承载了网络连接、服务调用和重试等功能。&lt;/p&gt;</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="dubbo" scheme="https://eoccc.gitee.io/tags/dubbo/"/>
    
  </entry>
  
  <entry>
    <title>kafka分区leader选举</title>
    <link href="https://eoccc.gitee.io/2022/04/25/kafka-leader%E9%80%89%E4%B8%BE/"/>
    <id>https://eoccc.gitee.io/2022/04/25/kafka-leader%E9%80%89%E4%B8%BE/</id>
    <published>2022-04-25T14:52:41.000Z</published>
    <updated>2022-08-21T13:46:47.327Z</updated>
    
    <content type="html"><![CDATA[<p>分区leader的选举由控制器负责实施。</p><span id="more"></span><h1 id="控制器"><a class="markdownIt-Anchor" href="#控制器"></a> 控制器</h1><p>kafka会在聚群中选择一个broker作为控制器（Controller），负责管理整个集群中所有分区和副本的状态。Controller的职责：</p><ul><li>当分区leader故障时，由控制器负责为该分区选举新的leader副本</li><li>当某个分区的ISR集合发生变化时，通知所有的broker更新元数据</li><li>当手动使用kafka-topics.sh脚本为某个topic增加分区时，重新分配分区</li></ul><h2 id="控制器的选举"><a class="markdownIt-Anchor" href="#控制器的选举"></a> 控制器的选举</h2><p>控制器选举工作依赖于 ZooKeeper，成功竞选为控制器的broker会在 ZooKeeper</p><p>中创建临时节点：/controller，包含如下信息：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;version&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;brokerId&quot;</span><span class="punctuation">:</span><span class="number">0</span><span class="punctuation">,</span><span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span><span class="string">&quot;1529210278988&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>kafka集群有且仅有一个controller，每个broker启动的时候，都会读取/controller节点的信息，如果brokerId不为-1，说明已经有controller了，放弃竞选，否则就会尝试创建/controller节点，创建成功则成为controller。</p><p>另外，kafka还在zookeeper中保存了一个持久节点：/controller_epoch，记录了controller的纪元。每个和控制器的交互都会带上controller_epoch，如果请求中的纪元小于kafka中的controller_epoch，则认为这个交互是无效的；如果大于kafka中的controller_epoch，则说明已经有新的controller当选。</p><h1 id="分区leader选举"><a class="markdownIt-Anchor" href="#分区leader选举"></a> 分区leader选举</h1><p>leader分区选举的触发时机：</p><blockquote><ol><li>创建主题</li><li>增加分区</li><li>原来的leader分区下线</li><li>手动触发分区重分配</li></ol></blockquote><p>选举分区的策略：按照AR集合中副本的顺序，找出第一个<strong>存活</strong>且<strong>存在于ISR中</strong>的分区，作为主分区。</p><p>注意：如果<code>unclean.leader.election.enable</code>配置为true，当ISR为空的时候，允许从非ISR中选择leader，即从AR中选择第一个存活的副本作为leader。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;分区leader的选举由控制器负责实施。&lt;/p&gt;</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="kafka" scheme="https://eoccc.gitee.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka副本管理</title>
    <link href="https://eoccc.gitee.io/2022/04/25/kafka%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86/"/>
    <id>https://eoccc.gitee.io/2022/04/25/kafka%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86/</id>
    <published>2022-04-25T14:52:41.000Z</published>
    <updated>2022-08-25T01:55:42.503Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka分区使用多副本机制来提升可靠性，只有leader副本能对外提供读写服务，follower副本只负责在内部进行消息同步，当leader副本不可用时，从follower副本中选择一个成为新的leader副本。</p><span id="more"></span><h1 id="优先副本"><a class="markdownIt-Anchor" href="#优先副本"></a> 优先副本</h1><p>在创建主题的时候，kafka会尽可能均匀地将分区和副本分布到各个broker节点上，leader副本的分布也比较均匀。</p><p>对于leader副本的选择，kafka会优先选择AR集合中的第一个副本。比如，分区0的AR集合为[1,2,0]，则kafka会优先将副本1作为这个分区的leader副本。</p><p>kafka提供了分区自动平衡的功能，通过参数<code>auto.leader.rebalance.enable</code>控制，默认为true。通过一个定时任务轮询所有的broker节点，计算每个broker的不平衡率（非优先副本的leader个数/分区总数），如果不平衡率超过<code>leader.imbalance.per.broker.percentage</code>配置的比例，就会触发优先副本的选举。执行的周期通过<code>leader.imbalance.check.interval.seconds </code>配置，默认为300秒。</p><p>需要注意的是，优先副本的选举会造成kafka不可用，生产环境不建议打开自动平衡副本。可以通过<code>kafka-perferr.replica.election.sh</code>脚本手动执行平衡动作。</p><h1 id="分区重分配"><a class="markdownIt-Anchor" href="#分区重分配"></a> 分区重分配</h1><p>当集群新增broker节点时，只有新创建的主题分区才会分配到这个节点上。因此集群扩容、或节点失效的时候，需要手动进行分区重分配，以保证分区的平衡。</p><p>Kafka 提供了 <a href="http://kafka-reassign-partitions.sh">kafka-reassign-partitions.sh</a> 脚本来执行分区重分配的工作。</p><p>分区重分配的本质在于数据复制，先增加新的副本，然后进行数据同步，最后删除旧的副本。复制会占用大量资源，如果分配的量太大，会影响性能，可以减小重分配的粒度，以小批次进行重分配。但是如果集群中某个主题或分区的流量特别大，仅靠减小粒度是不足够的，这时可以通过限流来实现。</p><p>副本间的复制限流有两种实现方式: <a href="http://kafka-config.sh">kafka-config.sh</a> 脚本和 <a href="http://kafka-reassign-partitions.sh">kafka-reassign-partitions.sh</a> 脚本。</p><h1 id="修改副本因子"><a class="markdownIt-Anchor" href="#修改副本因子"></a> 修改副本因子</h1><p>创建主题之后，如果我们想修改分区的个数，或者修改副本的个数，可以通过重分配的脚本 <a href="http://kafka-reassign-partition.sh">kafka-reassign-partition.sh</a> 来实现。</p><p>适当的增加分区数，会提高并发度，从而提高吞吐量，但是并不是分区数越多，吞吐量就会一直增长。</p><h1 id="isr副本伸缩"><a class="markdownIt-Anchor" href="#isr副本伸缩"></a> ISR副本伸缩</h1><p>正常情况下，所有副本都处于ISR集合中，但是当发生了异常的时候，某些副本会失效，从ISR中移除。</p><p>kafka从两个维度判断副本是否正常同步：</p><blockquote><ol><li>检查当前时间与副本最后同步时间是否超过<code>replica.lag.time.max.ms</code>指定的值</li><li>检查follower副本滞后的消息数是否超过<code>replica.lag.max.messages</code>配置的阈值</li></ol></blockquote><p>不过，从0.9x版本开始，kafka废除了<code>replica.lag.max.messages</code>这个参数，因为很难无法给定一个合适的值。如果设置的太大，则这个参数就没有太大的意义；如果设置的太小，则会让follower频繁处于同步与未同步的循环中，造成ISR集合频繁伸缩。而且这个参数是broker级别的，对这个broker上的所有分区生效，各个分区的TPS又不同，所以很难给一个合适的值。</p><p>对于新增加的副本，在赶山leader的HW之前是失效的。</p><h1 id="检查点"><a class="markdownIt-Anchor" href="#检查点"></a> 检查点</h1><p>线明确几个概念：</p><blockquote><ol><li>LEO：LogEndOffset的缩写，表示一个partition中最后一条log的offset</li><li>HW：HighWaterMark的缩写，为所有副本中LEO的最小值，消费者最多能消费到HW对应的offset</li></ol></blockquote><p>HW的更新：follower每次向leader请求同步数据，都会带上自己的LEO，leader取各个follower的LEO的最小值作为HW，然后将HW返回给follower，然后follower更新自己的HW。</p><p>Kafka 的根目录下有四个检查点文件：</p><blockquote><ul><li><p>cleaner-offset-checkpoint</p><p>记录每个partition中已经清理的日志偏移量</p></li><li><p>log-start-offset-checkpoint</p><p>标识副本中各个partition的日志起始偏移量，通过定时任务更新</p></li><li><p>recovery-point-offset-checkpoint</p><p>对应LEO，标识以及写入到磁盘的日志的offset，通过定时任务将LEO刷到检查点</p></li><li><p>replication-offset-checkpoint</p><p>对应HW，标识消费者可以消费的最高offset，通过定时任务将HW刷到检查点</p></li></ul></blockquote>]]></content>
    
    
    <summary type="html">leader副本选举，副本同步</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="kafka" scheme="https://eoccc.gitee.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>反射异常？类型转换的锅</title>
    <link href="https://eoccc.gitee.io/2022/04/23/%E5%8F%8D%E5%B0%84%E5%BC%82%E5%B8%B8%EF%BC%9F%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E7%9A%84%E9%94%85/"/>
    <id>https://eoccc.gitee.io/2022/04/23/%E5%8F%8D%E5%B0%84%E5%BC%82%E5%B8%B8%EF%BC%9F%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E7%9A%84%E9%94%85/</id>
    <published>2022-04-23T06:10:32.000Z</published>
    <updated>2022-08-12T08:21:07.171Z</updated>
    
    <content type="html"><![CDATA[<p>前两天一个同事上线了代码，然后就飙了一阵<code>java.lang.NoSuchMethodException</code>，一时还不知所以，看不出代码存在什么问题，拉了好几个人在看，最后查到时反射的获取方法的时候，对象的类型不匹配，导致异常。</p><span id="more"></span><p>代码类似如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Demo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Demo</span> <span class="variable">demo</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Demo</span>();</span><br><span class="line">        List&lt;String&gt; list = Lists.newArrayList(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>);</span><br><span class="line">        <span class="type">Method</span> <span class="variable">printElement</span> <span class="operator">=</span> Demo.class.getDeclaredMethod(<span class="string">&quot;printElement&quot;</span>, list.getClass());</span><br><span class="line">        printElement.invoke(demo, list);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">printElement</span><span class="params">(List&lt;String&gt; list)</span> &#123;</span><br><span class="line">        Assert.isTrue(list != <span class="literal">null</span>, <span class="string">&quot;list can not be null&quot;</span>);</span><br><span class="line">        list.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>乍一看不存在什么问题，日常写代码也是这么写的。但是运行的时候就是报了如下的错误：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> java.lang.NoSuchMethodException: com.my.Demo.printElement(java.util.ArrayList)</span><br></pre></td></tr></table></figure><p>问题就出在了通过反射获取class的方法这里：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Demo.class.getDeclaredMethod(<span class="string">&quot;printElement&quot;</span>, list.getClass());</span><br></pre></td></tr></table></figure><p>异常栈里面已经写的很清楚了，查找的是<code>printElement(java.util.ArrayList)</code>方法。随后我们对这段代码进行了分析。</p><p>以下是反射查找方法的核心方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Method <span class="title function_">searchMethods</span><span class="params">(Method[] methods,</span></span><br><span class="line"><span class="params">                                    String name,</span></span><br><span class="line"><span class="params">                                    Class&lt;?&gt;[] parameterTypes)</span>&#123;</span><br><span class="line">  <span class="type">ReflectionFactory</span> <span class="variable">fact</span> <span class="operator">=</span> getReflectionFactory();</span><br><span class="line">  <span class="type">Method</span> <span class="variable">res</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">for</span> (Method m : methods) &#123;</span><br><span class="line">    <span class="keyword">if</span> (m.getName().equals(name)</span><br><span class="line">        &amp;&amp; arrayContentsEq(parameterTypes,</span><br><span class="line">                           fact.getExecutableSharedParameterTypes(m))</span><br><span class="line">        &amp;&amp; (res == <span class="literal">null</span></span><br><span class="line">            || (res.getReturnType() != m.getReturnType()</span><br><span class="line">                &amp;&amp; res.getReturnType().isAssignableFrom(m.getReturnType()))))</span><br><span class="line">      res = m;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而对参数进行匹配的方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">arrayContentsEq</span><span class="params">(Object[] a1, Object[] a2)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (a1 == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="type">return</span> <span class="variable">a2</span> <span class="operator">=</span>= <span class="literal">null</span> || a2.length == <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (a2 == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> a1.length == <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (a1.length != a2.length) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; a1.length; i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (a1[i] != a2[i]) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>跟进去后发现a1（需要查找的参数）种存的类型为<code>class java.util.ArrayList</code>，而a2（class中的参数）存的类型为<code>interface java.util.List</code>，自然匹配不上。而且一个是<strong>class</strong>，一个是<strong>interface</strong> ！</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h1r49pcgrlj20py06e3za.jpg" alt="image-20220430014244128" style="zoom:50%;"><p>那么如何解决？我们都知道原因了，解决起来就很简单——只要保证查找的方法的参数类型和实际方法的参数类型一致就可以了，自然就有两个方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//方法一</span></span><br><span class="line"><span class="comment">//查找类型设置为List.class</span></span><br><span class="line">Demo.class.getDeclaredMethod(<span class="string">&quot;printElement&quot;</span>, List.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//方法二</span></span><br><span class="line"><span class="comment">//将类中的方法定义为ArrayList</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">printElement</span><span class="params">(ArrayList&lt;String&gt; list)</span> &#123;</span><br><span class="line">  Assert.isTrue(list != <span class="literal">null</span>, <span class="string">&quot;list can not be null&quot;</span>);</span><br><span class="line">  list.forEach(System.out::println);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>还有一个问题：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; list = Lists.newArrayList(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>);</span><br></pre></td></tr></table></figure><p>这一行代码我明明讲list定义为List类型了，为什么<code>list.getClass()</code>拿到的是<code>class java.util.ArrayList</code>?其原因是进行了<strong>向下转型</strong>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;前两天一个同事上线了代码，然后就飙了一阵&lt;code&gt;java.lang.NoSuchMethodException&lt;/code&gt;，一时还不知所以，看不出代码存在什么问题，拉了好几个人在看，最后查到时反射的获取方法的时候，对象的类型不匹配，导致异常。&lt;/p&gt;</summary>
    
    
    
    
    <category term="随笔" scheme="https://eoccc.gitee.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="踩坑" scheme="https://eoccc.gitee.io/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>kafka小知识</title>
    <link href="https://eoccc.gitee.io/2022/04/19/kafka%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    <id>https://eoccc.gitee.io/2022/04/19/kafka%E5%B0%8F%E7%9F%A5%E8%AF%86/</id>
    <published>2022-04-19T14:52:41.000Z</published>
    <updated>2022-08-01T04:58:42.277Z</updated>
    
    <content type="html"><![CDATA[<p>多生产者、多消费者、高性能、可伸缩、有消息堆积能力的 <strong>消息队列</strong> 。</p><span id="more"></span><p><a href="https://aidodoo.com/post/kafka/kafka%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">很好的Kafka学习网站</a></p><h2 id="kafka的常见名词概念"><a class="markdownIt-Anchor" href="#kafka的常见名词概念"></a> Kafka的常见名词概念</h2><ol><li><p><strong>Broker</strong><br>存储消息的服务器，一个Kafka集群由多个Broker组成</p></li><li><p><strong>Topic（主题）</strong><br>每条消息发到Broker上都有一个类别，即Topic。Topic是消息逻辑上的分类概念。</p></li><li><p><strong>Partition（分区）</strong><br>Partition是一个物理上的概念，每个Topic包含一个或多个Partition，每个Partition是一个文件夹。Partition的特点是 <strong>ordered &amp; immutable</strong>。每个partition都有一个broker为“leader”，其余都为“follower”。</p><blockquote><p>Topic、Partition与Replica</p><ol><li>Kafka使用 <strong>Topic</strong> 组织数据，每个主题有若干个 <strong>Partition</strong> ，每个 <strong>Partition</strong> 有多个 <strong>副本（Replica）</strong>；每个 <strong>Broker</strong> 可以保存成百上千个属于不同 <strong>Topic</strong> 的 <strong>副本（Replica）</strong>；</li><li>同一个Topic不保证消息有序性，同一个Partition保证消息有序性；</li><li><strong>Topic</strong> 是用户订阅时关注的逻辑概念，而 <strong>分区（Partition）</strong> 是具体存储的物理逻辑，当我们对消息有序有要求时，我们需要使用 <strong>分区器</strong> ，对消息的分区行为进行定义以满足要求；至于 <strong>副本</strong> ，这是Kafka对数据可用性、持久性做的机制，使用者不需关注。</li><li>Partition的 “伸缩” 支持了 <strong>海量数据的存储</strong>、<strong>数据的高并发读取</strong>、<strong>极高的吞吐量</strong> 。</li></ol></blockquote></li><li><p><strong>Replica（副本）</strong><br>保证系统、数据的可用性、可靠性。<br>4.1 “首领副本&quot;和&quot;跟随副本”<br>​<strong>首领副本</strong> ：每个Partition都有一个首领副本，读写都会经过此副本；<br>​<strong>跟随副本</strong> ：从 <strong>首领</strong> 那里复制消息，不处理客户端请求，<strong>首领</strong> 崩溃时，升级为新首领（控制器选取）。<br>4.2 “同步副本&quot;和&quot;不同步副本”<br>​同步副本列表（In-Sync Replica，ISR）<br>​    不同步副本列表（Outof-Sync Replicas，OSR）</p><p>4.3 副本存活的条件：</p><ol><li>副本节点必须能与zookeeper保持会话（心跳机制）</li><li>副本能复制leader上的所有写操作，并且不能落后太多</li></ol><p>4.4 一条消息只有被ISR中所有的节点同步完成，才算提交成功</p><p>4.5 只有已提交的消息才能被消费</p></li><li><p><strong>Message (消息)</strong></p><p>传递的数据对象，主要由四部分构成，其中 <strong>offset</strong> 和 <strong>timestamp</strong> 在kafka集群中产生，key/value在producer发送数据的时候产生：</p><pre><code>1. offset(偏移量)（消费者自己保存消费进度）  是一个long型整数，代表了这条消息在Partition文件中的偏移量，它是一条消息在文件中的唯一标识。2. key，按照key进行哈希来选择partition(默认)；key没有填的时候使用round-robin来选partition。3. value4. timestamp(插入时间)；</code></pre></li><li><p><strong>生产者</strong><br>负责发布消息到Kafka broker。</p></li><li><p><strong>消费者</strong><br>消息消费者，从Kafka broker读取消息的客户端。</p></li><li><p><strong>消费者组</strong><br>消费者组（Consumer Group），多个消费者组成一个消费者组，共同消费一个Topic，每个消费者消费一部分分区；</p><blockquote><ol><li>一个Partition分区只能被组中的一个消费者消费；</li><li>若消费者数目大于partition数目，则会有消费者空置；</li><li>每个Consumer属于一个特定的Consumer Group；</li><li>可为每个Consumer指定group name，若不指定group name则属于默认的group；</li><li>一个Topic可以被多个消费者或消费者群组共同并行读取。</li></ol></blockquote></li></ol><p>​8.1 消费者群组和分区再均衡<br>​         群组加入新的消费者，它会读取原本由其他消费者读取的分区；群组删除消费者，该消费者读取的分区会由其他消费者读取；这就是 <strong>分区再均衡</strong> 。再均衡为消费者群组带来了高可用性和伸缩性（放心增加、删除消费者）。</p><ol start="9"><li><p><strong>控制器</strong></p><p><strong>控制器</strong> 就是一个Broker，集群中有且仅有一个控制器。所有Broker通过向zookeeper注册临时节点竞争获得。这个Broker的意义是防止 <strong>羊群效应</strong>（Zookeeper通知的客户端过多，造成zookeeper性能突然下降，影响使用），负责管理整个集群中所有分区和副本的状态，如:</p><ol><li>负责头领副本的选举</li><li>某个Partition的ISR集合发生变化时，控制器负责通知所有Broker更新其元数据信息</li></ol></li><li><p><strong>ISR(In-Sync Replicas)</strong></p><p>leader会追踪和维护ISR中所有follower的滞后状态。如果滞后太多（时间滞后<code>replica.lag.time.max.ms</code>，消息滞后<code>replica.lag.max.messages</code>），leader会把该replica从ISR中移除。被移除ISR的replica一直在追赶leader。</p><p>leader写入数据后并不会commit，只有ISR列表中的所有folower同步之后才会commit，把滞后的follower移除ISR主要是避免写消息延迟。设置ISR主要是为了broker宕掉之后，重新选举partition的leader从ISR列表中选择。</p><p><code>min.insync.replicas</code>: 最小的ISR个数，当ISR个数小于配置的个数，leader将变成只读，直到replica同步消息追赶上leader。</p></li><li><p><strong>Zookeeper在kafka中的作用</strong><br>在Kafka集群中，Zookeeper负责 <strong>管理协调</strong> 、<strong>元数据</strong> 保存等工作。Broker、消费者和Zookeeper相连，而生产者只和Broker相连。</p></li><li><p>Kafka使用ZooKeeper用于管理、协调；Kafka控制器的选举由Zookeeper负责；启动时，第一个在Zookeeper创建一个临时节点/controller的会成为控制器；为了防止脑裂，每个控制器都有一个epoch字段，并且是递增的，<strong>收到消息的epoch小于当前控制器epoch则忽略</strong> 。</p></li></ol><blockquote><p>脑裂：失效的HA节点认为自己仍然有效，和当前的HA节点争抢&quot;共享服务&quot;或&quot;资源&quot;，造成数据损坏，服务失败等问题。</p><p>如果没有epoch，Kafka脑裂发生的情况：旧的Controller失去与Zookeeper的关联后，但这个Controller并没有意识到自己已经失效而且还保持着和其他Broker的连接，其他Broker尝试成为Controller；这时新旧Controller同时给Broker发送命令消息，造成脑裂。</p></blockquote><ol start="2"><li>Kafka将元数据信息保存在Zookeeper中，这包括<br>broker信息、topic信息、分区与消费者的关系、消费者消费进度(offset)等信息。<br>[Zookeeper在Kafka中作用](<a href="https://www.jianshu.com/p/a036405f989c">https://www.jianshu.com/p/a036405f989c</a>   zookeeper在kafka中作用)</li></ol><p><strong>一段话概括Kafka架构：</strong></p><p>Kafka由一个或多个Broker组成，消息以Partition为单位的log文件的形式存储于Broker中，每个Topic可以对应一个或多个Partition，这为消息提供了容量上的扩展性和消费上的并发度；为了保证消息的可靠性以及可用性，每个Partition都会有多个Replica。</p><h2 id="kafka文件存储机制"><a class="markdownIt-Anchor" href="#kafka文件存储机制"></a> Kafka文件存储机制</h2><h3 id="存储格式"><a class="markdownIt-Anchor" href="#存储格式"></a> 存储格式</h3><p>Topic是一个逻辑概念，Partition才是一个物理概念。</p><ul><li><p>一个Partition是一个目录，topic名+id（id从0开始）</p></li><li><p>Partition目录下包含多个大小相等 <strong>segment(段)数据文件</strong> ，这样做是为了方便删除</p></li><li><p>segment文件由两部分组成，分别为“.index”文件和“.log”文件</p><ul><li><p>文件名从0开始，后续的文件名是 <strong>上一个segment文件最后一条消息的offset值</strong></p></li><li><p>数据文件存储消息实体</p></li><li><p>索引文件存储对应数据文件部分消息的元数据（稀疏索引）</p><p>（索引文件中元数据指向对应数据文件中message的物理偏移地址）</p></li></ul></li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/2835676-f378607bc841309a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><h3 id="删除策略"><a class="markdownIt-Anchor" href="#删除策略"></a> 删除策略</h3><p>Kafka具备消息堆积能力，但消息不能无限增长。</p><p>默认规定：保留7天，或超过指定大小，如1G；达到上限时，数据会被删除。</p><h2 id="kafka副本同步机制"><a class="markdownIt-Anchor" href="#kafka副本同步机制"></a> Kafka副本同步机制</h2><p>Kafka同一个Partition下有多个Replica，并且分为Leader Replica和follower Replica。消息的读写都由Leader Replica完成。</p><p>Leader Replica还维护<strong>ISR列表</strong>，消费者只能消费到ISR中offset最小值（高水位HW）之前的消息（不是完全的同步，又不是完全的异步，在吞吐率和可靠性之间做了均衡）。</p><p>副本不同步的原因：</p><ul><li>慢副本：在一定周期时间内follower不能追赶上leader。最常见的原因之一是I / O瓶颈导致follower追加复制消息速度慢于从leader拉取速度。</li><li>卡住副本：在一定周期时间内follower停止从leader拉取请求。follower replica卡住了是由于GC暂停或follower失效或死亡。</li><li>新启动副本：当用户给主题增加副本因子时，新的follower不在同步副本列表中，直到他们完全赶上了leader日志。</li></ul><p><a href="https://aidodoo.com/post/kafka/kafka%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6/">副本同步机制参考</a></p><h2 id="kafka数据一致性和可靠性保证"><a class="markdownIt-Anchor" href="#kafka数据一致性和可靠性保证"></a> Kafka数据一致性和可靠性保证</h2><p>当producer向leader发送数据时，可以通过<code>request.required.acks</code>参数来设置数据可靠性的级别</p><ul><li>1（默认）：这意味着producer在ISR中的leader已成功收到的数据并得到确认后发送下一条message。如果leader宕机了，则会丢失数据。</li><li>0：这意味着producer无需等待来自broker的确认而继续发送下一批消息。这种情况下数据传输效率最高，但是数据可靠性确是最低的。</li><li>-1：producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。但是这样也不能保证数据不丢失，比如当ISR中只有leader时（前面ISR那一节讲到，ISR中的成员由于某些情况会增加也会减少，最少就只剩一个leader），这样就变成了acks=1的情况。</li><li>min.insync.replicas=2：ISR集合中最少有2个replica，否则写入失败。如果ISR成员数目小于2，该Partition的Leader Replica会变得可读，但不可写。只有被ISR中所有Replica同步的消息才被Commit，才能被消费者消费。</li></ul><h2 id="kafka-partition-leader选举"><a class="markdownIt-Anchor" href="#kafka-partition-leader选举"></a> Kafka Partition leader选举</h2><p>Kafka在Zookeeper中为每一个partition动态的维护了一个ISR，这个ISR里的所有replica都跟上了leader，只有ISR里的成员才能有被选为leader的可能。</p><p>（默认 unclean.leader.election.enable=false，非ISR中的副本不能够参与选举）</p><p>如果Leader Replica宕机，而ISR全部不可用，为了恢复业务，我们需要重启Leader和ISR，如果失败，则需要修改配置，unclean.leader.election.enable=true。</p><p>如果min.insync.replicas数目达不到，造成系统不可写，而又重启有失败，就要修改min.insync.replicas使系统可写。</p><h2 id="kafka消息幂等性"><a class="markdownIt-Anchor" href="#kafka消息幂等性"></a> Kafka消息幂等性</h2><p>主要考虑三种消息可靠性：</p><ul><li>At most once: 消息可能会丢，但绝不会重复传输</li><li>At least once：消息绝不会丢，但可能会重复传输</li><li>Exactly once：每条消息肯定会被传输一次且仅传输一次</li></ul><p>我们想要的是 <strong>Exactly once</strong> ，但在实际场景中，常常是 <strong>At least once</strong> 或者 <strong>At most once</strong> ，</p><ul><li>At least once：<ul><li>producer无法判断消息是否真正提交时，会多次retry，重复发送该消息；</li><li>consumer先消费消息，再commit，消费成功，但commit前crash了，下次还会接收到该消息；</li></ul></li><li>At most once：<ul><li>Producer的消息没发到Broker（request.required.acks=0）</li><li>consumer先commit，再消费消息，消费成功，但commit前crash了，下次还会接收到该消息；</li></ul></li></ul><p>所以，一般在使用中，我们会让消息处于At least once状态，然后在consumer处理时，根据业务的特性，进行幂等处理。</p><h2 id="kafka消息的有序性"><a class="markdownIt-Anchor" href="#kafka消息的有序性"></a> Kafka消息的有序性</h2><p>Kafka消息同一个Topic下的消息是无序的，同一Partition下的消息是有序的。</p><p>我们可以自定义分区器，使消息根据业务的具体特性进行分区。</p><h2 id="kafka运维"><a class="markdownIt-Anchor" href="#kafka运维"></a> Kafka运维</h2><p>硬件配置：</p><ul><li>磁盘吞吐量、磁盘容量、内存、网络、CPU、Broker数目等等</li></ul><h2 id="相关问题"><a class="markdownIt-Anchor" href="#相关问题"></a> 相关问题</h2><ul><li><p>kafka节点之间如何复制备份的？</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.将所有Broker（假设共n个Broker）和待分配的Partition排序</span><br><span class="line">2.将第i个Partition分配到第（i mod n）个Broker上 （这个就是leader）</span><br><span class="line">3.将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker</span><br></pre></td></tr></table></figure></li><li><p>kafka消息是否会丢失？为什么？</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="strong">**网络异常：**</span> <span class="code">`request.required.acks`</span>设置为0时，不和Kafka集群进行消息接受确认，当网络发生异常等情况时，存在消息丢失的可能；</span><br><span class="line"><span class="strong">**客户端异常：**</span> 异步发送时，消息并没有直接发送至Kafka集群，而是在Client端按一定规则缓存并批量发送。在这期间，如果客户端发生死机等情况，都会导致消息的丢失；</span><br><span class="line"><span class="strong">**缓冲区满了：**</span> 异步发送时，Client端缓存的消息超出了缓冲池的大小，也存在消息丢失的可能；</span><br><span class="line"><span class="strong">**Leader副本异常：**</span> acks设置为1时，Leader副本接收成功，Kafka集群就返回成功确认信息，而Follower副本可能还在同步。这时Leader副本突然出现异常，新Leader副本(原Follower副本)未能和其保持一致，就会出现消息丢失的情况；</span><br></pre></td></tr></table></figure></li><li><p>kafka最合理的配置是什么？</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="strong">**高可用配置:**</span> </span><br><span class="line">  topic配置：</span><br><span class="line"><span class="code">    replication.factor&gt;=3，即副本数至少是3个；</span></span><br><span class="line"><span class="code">    2 &lt;= min.insync.replicas &lt;= replication.factor，最小同步副本数码至少为2</span></span><br><span class="line"><span class="code">  broker的配置：</span></span><br><span class="line"><span class="code">    leader的选举条件unclean.leader.election.enable=false，非ISR中的副本不能够参与选举</span></span><br><span class="line"><span class="code">  producer的配置：</span></span><br><span class="line"><span class="code">    request.required.acks=-1(all)</span></span><br><span class="line"><span class="code">    producer.type=sync（指定消息发送是同步的）</span></span><br></pre></td></tr></table></figure></li><li><p>kafka的leader选举机制是什么？</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">所有Partition的Leader选举都由<span class="code">`controller`</span>决定。Kafka会在Zookeeper上针对每个Topic维护一个称为ISR（in-sync replica，已同步的partition副本）的集合。只有当这些副本都跟Leader中的副本同步了之后，kafka才会认为消息已提交，并反馈给消息的生产者。如果这个集合有增减，kafka会更新zookeeper上的记录。如果某个分区的Leader不可用，Kafka就会从ISR集合中选择一个副本作为新的Leader。</span><br><span class="line">如果Leader Replica宕机，而ISR全部不可用，为了恢复业务，我们需要重启Leader和ISR，如果失败，则需要修改配置，unclean.leader.election.enable=true，允许非同步的partition成为leader。</span><br><span class="line">如果min.insync.replicas数目达不到，造成系统不可写，而又重启有失败，就要修改min.insync.replicas使系统可写。</span><br></pre></td></tr></table></figure></li><li><p>kafka的消息保证有几种方式？</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">`request.required.acks`</span></span><br><span class="line"><span class="strong">**1(默认):**</span> leader收到消息就向producer确认</span><br><span class="line"><span class="strong">**0:**</span> producer不关心leader有没有收到消息</span><br><span class="line"><span class="strong">**-1:**</span> leader收到消息，并且同步partition同步完成后才向producer确认</span><br><span class="line"><span class="code">`min.insync.replicas`</span></span><br><span class="line">设置ISR集合中最少的replica个数。</span><br></pre></td></tr></table></figure></li><li><p>Kafka如何实现高伸缩性？</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="strong">**分区再均衡**</span>  群组加入新的消费者，它会读取原本由其他消费者读取的分区；群组删除消费者，该消费者读取的分区会由其他消费者读取</span><br></pre></td></tr></table></figure></li><li><p>Kafka消息的有序性如何？</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在Kafka中，消息是以消息队列的方式存在，存储在Topic中，Topic有多个partition。而每一条消息根据 Key 值来分配到不同的 partition 中。在一个 Partition 中，消息是以追加的形式写入的。如果一个 Consumer 读取一个 Partition，那么数据可以被有序的消费掉。</span><br><span class="line">方案一，kafka topic 只设置一个partition分区。kafka默认保证同一个partition分区内的消息是有序的，则可以设置topic只使用一个分区，这样消息就是全局有序，缺点是只能被consumer group里的一个消费者消费，降低了性能，不适用高并发的情况。</span><br><span class="line">方案二，producer将消息发送到指定partition分区。producer可以在发送消息时可以指定需要保证顺序的几条消息发送到同一个分区，这样消费者消费时，消息就是有序。</span><br></pre></td></tr></table></figure></li><li><p>producer 的写入流程</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> producer 先从 zookeeper 的 &quot;/brokers/.../state&quot; 节点找到该 partition 的 leader </span><br><span class="line"><span class="bullet">2.</span> producer 将消息发送给该 leader </span><br><span class="line"><span class="bullet">3.</span> leader 将消息写入本地 log </span><br><span class="line"><span class="bullet">4.</span> followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK </span><br><span class="line"><span class="bullet">5.</span> leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</span><br></pre></td></tr></table></figure></li><li><p>如何防止脑裂</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kafka中只有一个控制器controller负责分区的leader选举，同步broker的新增或删除消息，但有时由于网络问题，可能同时有两个broker认为自己是controller，这时候其他的broker就会发生脑裂。</span><br><span class="line">解决：每当新的controller产生的时候就会在zk中生成一个全新的、数值更大的controller epoch的标识，并同步给其他的broker进行保存，这样当第二个controller发送指令时，其他的broker就会自动忽略。</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">多生产者、多消费者、高性能、可伸缩、有消息堆积能力的 **消息队列**</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="kafka" scheme="https://eoccc.gitee.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka日志存储</title>
    <link href="https://eoccc.gitee.io/2022/04/15/kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/"/>
    <id>https://eoccc.gitee.io/2022/04/15/kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</id>
    <published>2022-04-15T14:52:41.000Z</published>
    <updated>2022-08-21T13:58:19.526Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka中，一个分区对应一个日志，为了防止日志过大，又引入了日志段的概念（LogSegment），将日志切分为多个日志段，以便于维护和清理。一个LogSegment对应磁盘上的一个日志文件和两个索引文件。</p><span id="more"></span><h1 id="日志布局"><a class="markdownIt-Anchor" href="#日志布局"></a> 日志布局</h1><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h5eqerq5fij20ro0hwmz1.jpg" alt="image-20220812204302654" style="zoom:50%;"><p>日志的文件夹的命名规则为：tpoic-partition。</p><p>向Log追加消息时是顺序写入的，只有最后一个LogSegment才能写入新的消息，最后一个LogSegment即活跃日志段。</p><p>日志段中包含一个 .log 的日志文件，一个 .index 的偏移量索引文件，一个 .timeIndex 的时间戳索引文件。日志文件和两个索引文件都是根据基准偏移量（固定为20位数字）命名的，即这个LogSegment的第一条日志的偏移量，如第一个LogSegment的基准偏移量为0，则日志文件名为 00000000000000000000.log。</p><p>LogSegment还包括 .deleted, .cleaned, .swap等临时文件，以及 .snapshot, .txnindex, leader-epoch-checkpoint等文件。</p><p>另外，kafka第一次启动的时候，还会创建以下文件：</p><blockquote><ol><li>cleaner-offset-checkpoint</li><li>log-start-offset-checkpoint</li><li>recovery-point offset-checkpoint</li><li>replicat ion-offset-checkpoint</li><li>meta.properties</li></ol></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h5eqew5nrlj20vi0sin18.jpg" alt="image-20220813135227232" style="zoom:50%;"><h1 id="日志清理"><a class="markdownIt-Anchor" href="#日志清理"></a> 日志清理</h1><p>Kafka通过三种策略来清理日志文件：基于时间、基于日志大小、基于日志起始偏移量。</p><h2 id="基于时间清理日志"><a class="markdownIt-Anchor" href="#基于时间清理日志"></a> 基于时间清理日志</h2><p>日志删除任务会检查当前日志文件中是否有保留时间超过阈值的日志分段集合，保留时间可以配置毫秒、分钟和小时，优先级依次降低：</p><blockquote><p>log.retention.hours  默认为168，即7天</p><p>log.retention.minutes</p><p><a href="http://log.retention.ms">log.retention.ms</a></p></blockquote><p>查找过期日志分段文件时，先从 .timeindex 时间戳索引文件中获取最后一条索引的时间戳，如果时间戳大于0，则取其值，否则取最后修改时间。</p><p>如果所有的日志分段都已经过期，会先切处一个新的日志分段作为活跃日志分段，再把过期的日志分段删除。</p><p>删除日志分段时，会先从Log对象所维护的日志分段跳跃表中移除待删除的日志分段，以保证没有线程会读取这些日志，然后对日志分段的所有文件添加上 .deleted 后缀，最后由 delete-file 删除任务删除这些文件。delete-file 任务的执行周期通过 <code>file . delete.delay .ms</code> 配置，默认为1分钟。</p><h2 id="基于大小清理日志"><a class="markdownIt-Anchor" href="#基于大小清理日志"></a> 基于大小清理日志</h2><p>如果Log总size大于 <code>log.retention.bytes</code> 配置的阈值（默认为-1，即不限制），则会先计算需要删除的日志文件的大小，即总size和阈值的差值，然后从第一个日志分段开始计算，找出需要删除的日志分段，然后由删除任务执行删除。</p><p>单个日志分段的大小由 <code>log.segment.bytes</code> 配置，默认为1GB。</p><h2 id="基于起始偏移量清理日志"><a class="markdownIt-Anchor" href="#基于起始偏移量清理日志"></a> 基于起始偏移量清理日志</h2><p>kafka有一个logStartOffset记录了日志文件的其实偏移量，一般是第一个日志文件的baseOffset，但是可能会改变。可以使用 KafkaAdminClient 的 deleteR巳cords()方法、使用 kafka-delete-records.sh脚本修改。</p><p>kafka会将偏移量小于logStartOffset的日志分段删除。如下图会将日志分段1和2删除。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h5eqf0o1p6j20my0c2q3q.jpg" alt="image-20220813163023112" style="zoom:45%;"><h2 id="日志压缩"><a class="markdownIt-Anchor" href="#日志压缩"></a> 日志压缩</h2><p>对于相同的key有不同value的消息，kafka只会保存最新的消息。kafka通过 cleaner-offset-checkpoint 文件来记录每个分区中已清理的偏移量。</p><p>活跃的日志分段不会参与日志压缩。</p><h1 id="零拷贝"><a class="markdownIt-Anchor" href="#零拷贝"></a> 零拷贝</h1><p>所谓的零拷贝是指将数据直接从磁盘文件复制到网卡设备中，而不需要经由应用程序，减少了内核和用户模式之间的上下文切换 。</p><p>一般情况下，如果我们要把一个数据发送给用户，会经过4次复制，进行了4次上下文切换：</p><blockquote><ol><li>调用 read() 方法，将文件中的内容被复制到内核模式下的 ReadBuffer 中；</li><li>CPU 控制将内核模式数据复制到用户模式下；</li><li>调用 write() 方法，将用户模式下的内容复制到内核模式下的 Socket Buffer 中；</li><li>将 SocketBuffer 中的数据复制到网卡设备中传迭。</li></ol></blockquote><p>在零拷贝中，直接将 SocketBuffer 中的数据复制到网卡设备中传迭，只需要进行2次复制，进行了2次上下文切换：</p><blockquote><ol><li>read() 方法，将文件的内容复制到内核模式下的 ReadBuffer 中；</li><li>将<strong>包含数据的位置和长度信息的文件描述符</strong>添加到 Socket Buffer 中；</li><li>将 SocketBuffer 中的数据复制到网卡设备中传迭。</li></ol></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;Kafka中，一个分区对应一个日志，为了防止日志过大，又引入了日志段的概念（LogSegment），将日志切分为多个日志段，以便于维护和清理。一个LogSegment对应磁盘上的一个日志文件和两个索引文件。&lt;/p&gt;</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="kafka" scheme="https://eoccc.gitee.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>redis学习10-集群</title>
    <link href="https://eoccc.gitee.io/2022/04/02/redis%E5%AD%A6%E4%B9%A010-%E9%9B%86%E7%BE%A4/"/>
    <id>https://eoccc.gitee.io/2022/04/02/redis%E5%AD%A6%E4%B9%A010-%E9%9B%86%E7%BE%A4/</id>
    <published>2022-04-02T07:10:35.000Z</published>
    <updated>2022-09-05T01:41:58.608Z</updated>
    
    <content type="html"><![CDATA[<p>Redis集群会将数据库分为16384个槽（slot），集群中的每个节点可以处理0～16384个槽，redis的每个键只会落在其中的一个槽中。当数据库中的每个槽都有redis节点处理的时候，集群才会处于可用状态。</p><span id="more"></span><p>Reids可以用以下命令让一个节点负责处理第0～5000个槽：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLUSTER ADDSLOT 0 1 2 3 ... 5000</span><br></pre></td></tr></table></figure><p>同时，节点会将自己负责处理的槽的信息发送给集群中的其他服务器，集群中的每个节点都知道每个槽是由哪个节点负责。</p><h2 id="执行命令"><a class="markdownIt-Anchor" href="#执行命令"></a> 执行命令</h2><p>Redis中的16384个槽都分配到redis节点处理后，集群就会进入上线状态，开始接收客户端的命令。当集群服务器收到有关数据库的命令后，不会立即开始处理，而是会先计算出命令中的键是处于哪个槽，然后进行分派：</p><blockquote><ul><li><p>如果键所在的槽由自己负责，则开始处理命令</p></li><li><p>如果键所在的槽不是由自己负责，则会向客户端回复一个<code>MOVED</code>命令，指引客户端转向正确的节点，并重新发送命令。（集群模式的客户端收到<code>MOVED</code>命令不会打印错误，只会转向正确的节点，然后打印转向信息）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MOVED &lt;slot&gt; &lt;ip&gt;:&lt;port&gt;</span><br></pre></td></tr></table></figure></li></ul></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h1t25ak149j20r20f4jsm.jpg" alt="image-20220501180000995" style="zoom:45%;"><p>Redis使用<code>CRC-16</code>算法对key进行散列：</p><blockquote><p>crc16(key) % 16384</p></blockquote><p>使用以下命令，可以计算cluster会将key散列到哪个slot中去：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLUSTER KEYSLOT myKey</span><br></pre></td></tr></table></figure><h2 id="重新分片"><a class="markdownIt-Anchor" href="#重新分片"></a> 重新分片</h2><p>Redis集群可以把任意数量已指派给某一节点的槽指派给另一个节点，并且这些槽相关的键也会移到新的节点上。这个操作可以在线进行，并且在重新分片过程中，源节点和目标节点都可以执行命令。</p><h3 id="迁移过程"><a class="markdownIt-Anchor" href="#迁移过程"></a> 迁移过程</h3><p>重新分片由redis集群管理软件<strong>redis-trib</strong>负责操作，单个slot的迁移分为几个步骤：</p><blockquote><ol><li>向目标节点发送<code>CLUSTER SETSLOT &lt;slot&gt; IMPORTING &lt;source_id&gt;</code>命令，让目标节点准备好导入新的槽</li><li>向源节点发送<code>CLUSTER SETSLOT &lt;slot&gt; MAGRATING &lt;target_id&gt;</code>命令，让源节点准备好迁移槽</li><li>向源节点发送<code>CLUSTER  GETKEYSINSLOT &lt;slot&gt; &lt;count&gt;</code>命令，获得最多count个属于slot的键</li><li>根据3步骤中获得的键，向源节点发送<code>MIGRATE &lt;target_ip&gt; &lt;target_port&gt; &lt;key_name&gt; 0 &lt;timeout&gt;</code>命令，将键key_name迁移到目标节点</li><li>重复3、4步骤，将属于slot槽的所有键迁移到新节点</li><li>向集群中的任一节点发送<code>CLUSTER SETSLOT &lt;slot&gt; NODE &lt;target_id&gt;</code>命令，将slot槽指派给目标节点，这一指派信息会通过消息发送至集群中的每一个节点</li><li>集群中的节点收到新的指派消息后，更新分片信息</li></ol></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h1t4lrrchyj20wq08q0tp.jpg" alt="image-20220501192528800" style="zoom:50%;"><h3 id="迁移过程中访问键ask"><a class="markdownIt-Anchor" href="#迁移过程中访问键ask"></a> 迁移过程中访问键（ASK）</h3><p>在迁移过程中，如果客户端向源节点发送一个与数据库有关的命令，而且命令中的键正好在迁移的slot中时：</p><blockquote><ol><li>源节点会在自己的数据库中查找命令中的键，如果找到，执行命令</li><li>如果在源节点的数据库中没有找到命令中的键，说明这个键已经迁移到目标节点，这时源节点会回复一个<code>ASK</code>错误，指引客户端将命令发送给目标节点（客户端会忽略异常信息，只执行转向操作）</li></ol></blockquote><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h5vhclc46bj20o40ast9q.jpg" alt="image-20220501193151480" style="zoom:50%;"><p>客户端收到<code>ASK</code>错误后，会执行<code>ASKING</code>命令，打开客户端中的<code>REDIS_ASKING</code>标识，然后将新的命令发送给<code>ASK</code>错误信息中的目标节点。如果<code>REDIS_ASKING</code>标识没有打开，目标节点会拒绝执行命令，并且返回一个<code>MOVED</code>错误。</p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h1t4z8t1vpj20n60dkq46.jpg" alt="image-20220501193815198" style="zoom:45%;"><h2 id="复制与故障转移"><a class="markdownIt-Anchor" href="#复制与故障转移"></a> 复制与故障转移</h2><p>集群中的主节点负责处理槽，从节点复制主节点。当某个主节点下线时，从其从节点中选择一个成为新的主节点，代替主节点接收并处理客户端命令。</p><p>通过以下命令，让一个节点成为主节点的从节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLUSTER REPLICATE &lt;node_id&gt;</span><br></pre></td></tr></table></figure><h3 id="故障检测"><a class="markdownIt-Anchor" href="#故障检测"></a> 故障检测</h3><p>集群中的每个节点都会定期地向其他节点发送<code>PING</code>命令，检测其他节点是否存活。如果某个节点在指定的时间没没有返回<code>PONG</code>消息，则会认为这个服务器<strong>疑似下线</strong>，然后这个服务器会询问其他<font color="red">主节点</font>是否支持目标服务器下线，当收到半数以上的赞成票时，就会将目标服务器标记为<strong>已下线</strong>，然后会向集群广播一条这个主节点下线的消息。</p><h3 id="故障转移"><a class="markdownIt-Anchor" href="#故障转移"></a> 故障转移</h3><p>当一个<font color="red">从节点</font>发现自己的主节点下线时，将开始故障转移，从其从节点中选择一个节点成为新的主节点：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 在从节点中选择一个节点成为主节点</span><br><span class="line"><span class="bullet">2.</span> 被选中的节点执行<span class="code">`SLAVEOF no one`</span>命令，成为主节点</span><br><span class="line"><span class="bullet">3.</span> 新的主节点撤销旧的主节点的所有槽指派，并将这些槽指派给自己</span><br><span class="line"><span class="bullet">4.</span> 新主节点在集群中广播一条<span class="code">`PONE`</span>消息，让集群中的其他节点知道自己成为了新的主节点，并已接管了下线节点的槽</span><br><span class="line"><span class="bullet">5.</span> 新主节点开始接收和处理请求</span><br></pre></td></tr></table></figure><h3 id="选举新的主节点"><a class="markdownIt-Anchor" href="#选举新的主节点"></a> 选举新的主节点</h3><p>当发现某个服务器客观下线，就会开始故障转移，故障转移时要先选择一个新的主节点。选举的方式和Sentinel相似：</p><blockquote><ol><li>集群的纪元自增1</li><li>在一个纪元中，集群中的每个<font color="red">主节点</font>有一次投票机会</li><li>当从节点发现自己的主节点下线时，会向集群广播一条消息，并要求有投票权的主节点给自己投票</li><li>如果一个主节点有投票权，且在这个纪元中没有将票投给其他节点，会将票投给第一个请求投票的从节点（先到先得）</li><li>当一个从节点获得的票数过半时，当选成新的主节点</li><li>如果在一个纪元里，没有选举出新的主节点，会进入一个新的纪元（纪元加1），重新开始投票，直到选出新主节点</li></ol></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;Redis集群会将数据库分为16384个槽（slot），集群中的每个节点可以处理0～16384个槽，redis的每个键只会落在其中的一个槽中。当数据库中的每个槽都有redis节点处理的时候，集群才会处于可用状态。&lt;/p&gt;</summary>
    
    
    
    
    <category term="中间件" scheme="https://eoccc.gitee.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    <category term="redis" scheme="https://eoccc.gitee.io/tags/redis/"/>
    
  </entry>
  
</feed>
